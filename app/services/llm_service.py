"""
LLMç”ŸæˆæœåŠ¡ - è´Ÿè´£ä¸Qwen APIäº¤äº’ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”
"""

import asyncio
import json
from typing import Dict, List, Optional, AsyncGenerator
from datetime import datetime

import dashscope
from dashscope import Generation
from app.core.config import settings
from app.core.logger import get_logger

logger = get_logger(__name__)


class LLMService:
    """
    LLMç”ŸæˆæœåŠ¡ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    è´Ÿè´£ï¼šPromptæ„å»ºã€APIè°ƒç”¨ã€æµå¼å“åº”ã€é”™è¯¯å¤„ç†
    """
    
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(LLMService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if getattr(self, "_is_initialized", False):
            return
        
        logger.info("ğŸ”„ åˆå§‹åŒ–LLMç”ŸæˆæœåŠ¡...")
        
        # é…ç½®APIå¯†é’¥
        dashscope.api_key = settings.llm.api_key
        
        # æ¨¡å‹é…ç½®
        self.model_name = settings.llm.model_name
        self.temperature = settings.llm.temperature
        self.max_tokens = settings.llm.max_tokens
        self.top_p = settings.llm.top_p
        
        # ç³»ç»ŸPromptï¼ˆæ”¿åŠ¡é¢†åŸŸä¼˜åŒ–ï¼‰
        self.system_prompt = self._build_system_prompt()
        
        # ç¼“å­˜æœ€è¿‘å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
        self.conversation_cache = {}
        
        self._is_initialized = True
        logger.info(f"âœ… LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
    
    def _build_system_prompt(self) -> str:
        """æ„å»ºæ”¿åŠ¡é¢†åŸŸä¸“ç”¨ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€åæ”¿åŠ¡é—®ç­”åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”æ³¸å·å¸‚ç›¸å…³çš„æ”¿ç­–å’¨è¯¢å’Œæ°‘ç”Ÿé—®é¢˜ã€‚

# ä½ çš„è§’è‰²å’Œèƒ½åŠ›ï¼š
1. **æ”¿ç­–ä¸“å®¶**ï¼šç†Ÿæ‚‰æ³¸å·å¸‚å„çº§æ”¿åºœéƒ¨é—¨èŒè´£å’Œä¸šåŠ¡æµç¨‹
2. **ä¿¡æ¯æ•´åˆè€…**ï¼šåŸºäºæä¾›çš„æ¡ˆä¾‹ä¿¡æ¯ï¼Œå‡†ç¡®ã€å®Œæ•´åœ°å›ç­”ç”¨æˆ·é—®é¢˜
3. **ä¸“ä¸šæ²Ÿé€šè€…**ï¼šè¯­è¨€æ­£å¼ã€å‡†ç¡®ã€å‹å¥½ï¼Œç¬¦åˆæ”¿åŠ¡æ²Ÿé€šè§„èŒƒ

# å›ç­”è¦æ±‚ï¼š
1. **å‡†ç¡®æ€§ç¬¬ä¸€**ï¼šåªåŸºäºæä¾›çš„æ¡ˆä¾‹ä¿¡æ¯å›ç­”ï¼Œä¸ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯
2. **æ¸…æ™°æ ‡æ³¨æ¥æº**ï¼šå›ç­”ä¸­æ³¨æ˜å‚è€ƒçš„éƒ¨é—¨å’Œæ¡ˆä¾‹æ—¶é—´
3. **ç»“æ„åŒ–è¾“å‡º**ï¼šå¤æ‚é—®é¢˜åˆ†ç‚¹è¯´æ˜ï¼Œå…³é”®ä¿¡æ¯çªå‡º
4. **æ—¶æ•ˆæ€§è¯´æ˜**ï¼šæ³¨æ˜æ”¿ç­–æˆ–ä¿¡æ¯çš„æœ‰æ•ˆæ—¶é—´èŒƒå›´
5. **æä¾›åç»­æŒ‡å¼•**ï¼šç»™å‡ºç›¸å…³éƒ¨é—¨çš„è”ç³»æ–¹å¼æˆ–è¿›ä¸€æ­¥å’¨è¯¢é€”å¾„

# æ³¨æ„äº‹é¡¹ï¼š
- å¦‚æœæ¡ˆä¾‹ä¿¡æ¯ä¸è¶³æˆ–ä¸é—®é¢˜ä¸ç›¸å…³ï¼Œå¦‚å®å‘ŠçŸ¥ç”¨æˆ·
- æ¶‰åŠä¸ªäººéšç§æˆ–æ•æ„Ÿä¿¡æ¯æ—¶ï¼Œæç¤ºç”¨æˆ·é€šè¿‡æ­£è§„æ¸ é“å’¨è¯¢
- ä¸åŒéƒ¨é—¨çš„æ”¿ç­–å¯èƒ½ä¸åŒï¼Œæ³¨æ„åŒºåˆ†è¯´æ˜

ç°åœ¨å¼€å§‹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"""
    
    def build_rag_prompt(self, query: str, context: str, history: List[Dict] = None) -> str:
        """
        æ„å»ºRAGä¸“ç”¨Prompt
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å®Œæ•´çš„Promptå­—ç¬¦ä¸²
        """
        # åŸºç¡€Promptç»“æ„
        prompt_parts = []
        
        # 1. ç³»ç»Ÿæç¤º
        prompt_parts.append(f"ç³»ç»ŸæŒ‡ä»¤ï¼š{self.system_prompt}")
        prompt_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        # 2. å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if history and len(history) > 0:
            prompt_parts.append("å¯¹è¯å†å²ï¼š")
            for i, turn in enumerate(history[-3:]):  # åªä¿ç•™æœ€è¿‘3è½®
                role = "ç”¨æˆ·" if turn["role"] == "user" else "åŠ©æ‰‹"
                prompt_parts.append(f"{role}: {turn['content']}")
            prompt_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        # 3. æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        prompt_parts.append("ç›¸å…³æ¡ˆä¾‹ä¿¡æ¯ï¼š")
        prompt_parts.append(context)
        prompt_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        # 4. å½“å‰æŸ¥è¯¢
        prompt_parts.append("ç”¨æˆ·é—®é¢˜ï¼š")
        prompt_parts.append(query)
        prompt_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        # 5. å›ç­”è¦æ±‚ï¼ˆå†æ¬¡å¼ºè°ƒï¼‰
        prompt_parts.append("è¯·æ ¹æ®ä»¥ä¸Šæ¡ˆä¾‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œè¦æ±‚ï¼š")
        prompt_parts.append("1. å‡†ç¡®å¼•ç”¨æ¡ˆä¾‹ä¸­çš„å…·ä½“ä¿¡æ¯")
        prompt_parts.append("2. æ³¨æ˜ä¿¡æ¯æ¥æºï¼ˆéƒ¨é—¨ã€æ—¶é—´ï¼‰")
        prompt_parts.append("3. å¦‚æœä¿¡æ¯ä¸è¶³æˆ–ä¸ç¡®å®šï¼Œå¦‚å®è¯´æ˜")
        prompt_parts.append("4. ä½¿ç”¨æ­£å¼ã€ä¸“ä¸šçš„æ”¿åŠ¡è¯­è¨€")
        
        return "\n".join(prompt_parts)
    
    async def generate_response(
        self, 
        query: str, 
        context: str, 
        history: List[Dict] = None,
        stream: bool = False
    ) -> Dict[str, any]:
        """
        ç”Ÿæˆå›ç­”ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            history: å¯¹è¯å†å²
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            {
                "answer": str,           # ç”Ÿæˆçš„å›ç­”
                "usage": Dict,           # tokenä½¿ç”¨æƒ…å†µ
                "model": str,            # ä½¿ç”¨çš„æ¨¡å‹
                "finish_reason": str,    # ç»“æŸåŸå› 
                "timestamp": str         # ç”Ÿæˆæ—¶é—´
            }
        """
        start_time = datetime.now()
        
        try:
            # æ„å»ºPrompt
            prompt = self.build_rag_prompt(query, context, history)
            
            # è®°å½•æ—¥å¿—ï¼ˆç”Ÿäº§ç¯å¢ƒå¯æ§åˆ¶é•¿åº¦ï¼‰
            logger.debug(f"ç”ŸæˆPrompté•¿åº¦: {len(prompt)}å­—ç¬¦")
            
            # è°ƒç”¨Qwen API
            if stream:
                return await self._generate_stream(prompt)
            else:
                return await self._generate_once(prompt, start_time)
                
        except Exception as e:
            logger.error(f"âŒ LLMç”Ÿæˆå¤±è´¥: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _generate_once(self, prompt: str, start_time: datetime) -> Dict[str, any]:
        """ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å›ç­”"""
        response = Generation.call(
            model=self.model_name,
            prompt=prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            top_p=self.top_p,
            result_format='message'  # è¿”å›ç»“æ„åŒ–æ¶ˆæ¯
        )
        
        if response.status_code == 200:
            answer = response.output.choices[0].message.content
            
            return {
                "answer": answer,
                "usage": {
                    "prompt_tokens": response.usage.input_tokens,
                    "completion_tokens": response.usage.output_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "model": self.model_name,
                "finish_reason": response.output.choices[0].finish_reason,
                "response_time": (datetime.now() - start_time).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
    
    async def _generate_stream(self, prompt: str) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        response = Generation.call(
            model=self.model_name,
            prompt=prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            top_p=self.top_p,
            stream=True,  # å¯ç”¨æµå¼
            result_format='message'
        )
        
        for chunk in response:
            if chunk.status_code == 200:
                if hasattr(chunk.output, 'choices') and chunk.output.choices:
                    content = chunk.output.choices[0].message.content
                    if content:
                        yield content
            else:
                yield f"é”™è¯¯: {chunk.code} - {chunk.message}"
    
    def evaluate_response_quality(self, answer: str, context: str) -> Dict[str, float]:
        """
        ç®€å•è¯„ä¼°å›ç­”è´¨é‡ï¼ˆå¯æ‰©å±•ï¼‰
        
        Returns:
            è´¨é‡è¯„åˆ†å­—å…¸
        """
        scores = {
            "relevance": 0.8,  # ç›¸å…³æ€§ï¼ˆå¯æ ¹æ®å†…å®¹è®¡ç®—ï¼‰
            "completeness": 0.7,  # å®Œæ•´æ€§
            "accuracy": 0.9,  # å‡†ç¡®æ€§
            "formality": 0.8,  # æ­£å¼ç¨‹åº¦
        }
        
        # ç®€å•å¯å‘å¼è¯„åˆ†ï¼ˆåç»­å¯æ‰©å±•ä¸ºæ¨¡å‹è¯„ä¼°ï¼‰
        if "æ ¹æ®ä»¥ä¸Šæ¡ˆä¾‹" in answer or "å‚è€ƒ" in answer:
            scores["groundedness"] = 0.8  # åŸºäºä¸Šä¸‹æ–‡çš„ç¨‹åº¦
        
        if "éƒ¨é—¨" in answer and "æ—¶é—´" in answer:
            scores["attribution"] = 0.9  # æ¥æºæ ‡æ³¨
        
        return scores


# å·¥å…·å‡½æ•°ï¼šè·å–æœåŠ¡å®ä¾‹
def get_llm_service() -> LLMService:
    """è·å–LLMæœåŠ¡å•ä¾‹å®ä¾‹"""
    return LLMService()


# å·¥å…·å‡½æ•°ï¼šç”Ÿæˆå®Œæ•´RAGå›ç­”
async def generate_rag_response(
    query: str, 
    context: str, 
    history: List[Dict] = None
) -> Dict[str, any]:
    """
    å¿«é€Ÿç”ŸæˆRAGå›ç­”ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Example:
        result = await generate_rag_response("é›¨éœ²è®¡åˆ’", context_text)
        print(result["answer"])
    """
    service = get_llm_service()
    return await service.generate_response(query, context, history)