"""
Agentic RAGåè°ƒå¼•æ“ - æ ¸å¿ƒå†³ç­–ä¸æµç¨‹æ§åˆ¶
å¢å¼ºèƒ½åŠ›ï¼šAgentå†³ç­–é©±åŠ¨ã€å¤šç­–ç•¥æ£€ç´¢ã€ç»“æœè¯„ä¼°ã€è‡ªåŠ¨é‡æ£€ç´¢
"""

import time
from typing import Dict, List, Optional, Tuple
from app.services.retriever import HybridVectorRetriever
from app.services.llm_service import LLMService, AgentDecision, get_llm_service
from app.core.logger import get_logger

logger = get_logger(__name__)


class AgenticRAGEngine:
    """
    Agentic RAGåè°ƒå¼•æ“
    æ ¸å¿ƒæµç¨‹ï¼š
    1. Agentæ„å›¾åˆ†æä¸å†³ç­–
    2. åŸºäºå†³ç­–çš„å¤šç­–ç•¥æ£€ç´¢
    3. æ£€ç´¢ç»“æœè´¨é‡è¯„ä¼°
    4. æ™ºèƒ½ç”Ÿæˆä¸è´¨é‡æ ¡éªŒ
    5. è‡ªåŠ¨é‡æ£€ç´¢ï¼ˆå¯é€‰ï¼‰
    """

    def __init__(self):
        self.retriever = HybridVectorRetriever()
        self.llm_service: LLMService = get_llm_service()
        self.min_quality_score = 0.7  # æœ€ä½å›ç­”è´¨é‡é˜ˆå€¼
        logger.info("ğŸ¤– Agentic RAGå¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def _evaluate_retrieval_quality(
        self, query: str, results: List[Dict], metadata: Dict
    ) -> Dict[str, any]:
        """
        Agentæ ¸å¿ƒèƒ½åŠ›ï¼šè¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡
        æ£€æŸ¥ï¼šç›¸å…³æ€§ã€è¦†ç›–åº¦ã€æƒå¨æ€§
        """
        if not results:
            return {
                "retrieval_quality_score": 0.0,
                "suggestion": "æ— æ£€ç´¢ç»“æœï¼Œå»ºè®®æ‰©å¤§æ£€ç´¢èŒƒå›´",
                "need_reretrieval": True,
            }

        # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        avg_similarity = metadata.get("avg_similarity", 0.0)
        num_results = metadata.get("num_results", 0)
        dept_coverage = len(set([r["entity"].get("department", "") for r in results]))

        # ç»¼åˆæ£€ç´¢è´¨é‡è¯„åˆ†
        retrieval_score = (
            avg_similarity * 0.6  # ç›¸ä¼¼åº¦æƒé‡60%
            + min(1.0, num_results / 10) * 0.2  # æ•°é‡æƒé‡20%
            + min(1.0, dept_coverage / 5) * 0.2  # éƒ¨é—¨è¦†ç›–åº¦æƒé‡20%
        )

        # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ£€ç´¢
        need_reretrieval = retrieval_score < 0.6 or (  # ä½è´¨é‡æ£€ç´¢ç»“æœ
            num_results < 2 and avg_similarity < 0.7
        )  # ç»“æœå°‘ä¸”ç›¸ä¼¼åº¦ä½

        return {
            "retrieval_quality_score": retrieval_score,
            "avg_similarity": avg_similarity,
            "num_results": num_results,
            "dept_coverage": dept_coverage,
            "suggestion": "éœ€è¦é‡æ£€ç´¢" if need_reretrieval else "æ£€ç´¢ç»“æœåˆæ ¼",
            "need_reretrieval": need_reretrieval,
        }

    async def _reretrieve_with_adjusted_params(
        self, query: str, decision: AgentDecision
    ) -> Tuple[str, List[Dict], Dict]:
        """
        è‡ªåŠ¨é‡æ£€ç´¢ï¼ˆè°ƒæ•´å‚æ•°ï¼‰
        """
        logger.info(f"ğŸ”„ æ‰§è¡Œé‡æ£€ç´¢ï¼Œè°ƒæ•´æ£€ç´¢å‚æ•°...")

        # è°ƒæ•´æ£€ç´¢å‚æ•°ï¼ˆæ‰©å¤§èŒƒå›´ï¼‰
        adjusted_params = decision.retrieval_params.copy()
        adjusted_params["top_k"] = min(
            10, adjusted_params["top_k"] * 2
        )  # top_kç¿»å€ï¼ˆæœ€å¤§10ï¼‰
        adjusted_params["threshold"] = max(
            0.4, adjusted_params["threshold"] * 0.8
        )  # é˜ˆå€¼é™ä½20%ï¼ˆæœ€ä½0.4ï¼‰

        # æ‰§è¡Œé‡æ£€ç´¢
        context_str, results, metadata = self.retriever.retrieve(
            query=decision.query_rewritten or query, top_k=adjusted_params["top_k"]
        )

        # æ›´æ–°å…ƒæ•°æ®
        metadata["reretrieval"] = True
        metadata["adjusted_params"] = adjusted_params

        logger.info(f"âœ… é‡æ£€ç´¢å®Œæˆï¼Œæ–°å‚æ•°: {adjusted_params}, ç»“æœæ•°: {len(results)}")

        return context_str, results, metadata

    async def query(
        self,
        query: str,
        history: List[Dict] = None,
        stream: bool = False,
        enable_reretrieval: bool = True,
    ) -> Dict[str, any]:
        """
        Agentic RAGå®Œæ•´æŸ¥è¯¢æµç¨‹

        Returns:
            {
                "answer": str,               # ç”Ÿæˆçš„å›ç­”
                "sources": List[Dict],       # æ£€ç´¢åˆ°çš„æ¥æº
                "context": str,              # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
                "metadata": Dict[str, any],  # å…ƒæ•°æ®
                "generation_metrics": Dict,  # ç”Ÿæˆç›¸å…³æŒ‡æ ‡
                "agent_decision": Dict,      # Agentå†³ç­–ç»“æœ
                "quality_check": Dict        # è´¨é‡æ ¡éªŒç»“æœ
            }
        """
        start_time = time.time()
        final_result = {
            "query": query,
            "answer": "",
            "sources": [],
            "context": "",
            "metadata": {},
            "generation_metrics": {},
            "agent_decision": {},
            "quality_check": {},
        }

        try:
            # ========== Step 1: Agentæ„å›¾åˆ†æä¸å†³ç­– ==========
            logger.info(f"ğŸ§  Agentåˆ†ææŸ¥è¯¢æ„å›¾: {query}")
            decision: AgentDecision = await self.llm_service.analyze_query_intent(
                query, history
            )
            final_result["agent_decision"] = decision.model_dump()

            # ç›´æ¥å›ç­”ï¼ˆæ— éœ€æ£€ç´¢ï¼‰
            if decision.decision_type == "direct_answer":
                logger.info(f"ğŸ’¡ Agentå†³ç­–ï¼šç›´æ¥å›ç­”ï¼Œæ— éœ€æ£€ç´¢")
                # ç›´æ¥ç”Ÿæˆå›ç­”
                generation_result = await self.llm_service.generate_response(
                    query=query,
                    context="æ— éœ€æ£€ç´¢çš„é€šç”¨æ”¿åŠ¡é—®é¢˜",
                    history=history,
                    decision=decision,
                    stream=stream,
                )

                final_result.update(
                    {
                        "answer": generation_result["answer"],
                        "generation_metrics": generation_result,
                        "quality_check": generation_result.get("quality_check", {}),
                    }
                )
                return final_result

            # æ— æ³•å›ç­”
            if decision.decision_type == "cannot_answer":
                logger.info(f"ğŸš« Agentå†³ç­–ï¼šæ— æ³•å›ç­”è¯¥é—®é¢˜")
                final_result["answer"] = (
                    "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚è¯·ç¡®è®¤é—®é¢˜æ˜¯å¦å±äºæ³¸å·å¸‚æ”¿åŠ¡èŒƒç•´ï¼Œæˆ–é€šè¿‡æ”¿åŠ¡æœåŠ¡çƒ­çº¿12345å’¨è¯¢ã€‚"
                )
                final_result["quality_check"] = {"overall_score": 0.0}
                return final_result

            # ========== Step 2: åŸºäºAgentå†³ç­–çš„æ£€ç´¢ ==========
            logger.info(
                f"ğŸ” Agenté©±åŠ¨æ£€ç´¢ï¼Œç­–ç•¥: {decision.retrieval_strategy}, å‚æ•°: {decision.retrieval_params}"
            )

            # æ‰§è¡Œæ£€ç´¢
            context_str, results, metadata = self.retriever.retrieve(
                query=decision.query_rewritten or query,
                top_k=decision.retrieval_params.get("top_k", 5),
            )

            retrieval_time = time.time() - start_time
            metadata["retrieval_time"] = retrieval_time
            logger.info(
                f"âœ… åˆå§‹æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {retrieval_time:.2f}sï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ"
            )

            # ========== Step 3: æ£€ç´¢ç»“æœè´¨é‡è¯„ä¼° ==========
            retrieval_quality = await self._evaluate_retrieval_quality(
                query, results, metadata
            )
            metadata["retrieval_quality"] = retrieval_quality
            logger.info(
                f"ğŸ“Š æ£€ç´¢è´¨é‡è¯„åˆ†: {retrieval_quality['retrieval_quality_score']:.3f}"
            )

            # è‡ªåŠ¨é‡æ£€ç´¢ï¼ˆå¦‚æœå¼€å¯ä¸”éœ€è¦ï¼‰
            if enable_reretrieval and retrieval_quality["need_reretrieval"]:
                context_str, results, metadata = (
                    await self._reretrieve_with_adjusted_params(query, decision)
                )
                # é‡æ–°è¯„ä¼°é‡æ£€ç´¢ç»“æœ
                retrieval_quality = await self._evaluate_retrieval_quality(
                    query, results, metadata
                )

            # ========== Step 4: æ™ºèƒ½ç”Ÿæˆå›ç­” ==========
            logger.info(f"ğŸ¤– Agentå¼€å§‹ç”Ÿæˆå›ç­”...")
            generation_start = time.time()

            if stream:
                # æµå¼ç”Ÿæˆï¼ˆè¿”å›ç”Ÿæˆå™¨ï¼‰
                final_result["stream_generator"] = (
                    await self.llm_service.generate_response(
                        query=query,
                        context=context_str,
                        history=history,
                        decision=decision,
                        stream=stream,
                    )
                )
            else:
                # æ™®é€šç”Ÿæˆ
                generation_result = await self.llm_service.generate_response(
                    query=query, context=context_str, history=history, decision=decision
                )

                generation_time = time.time() - generation_start
                metadata["generation_time"] = generation_time

                # ========== Step 5: æ•´åˆæœ€ç»ˆç»“æœ ==========
                total_time = time.time() - start_time

                final_result.update(
                    {
                        "answer": generation_result["answer"],
                        "sources": results[: decision.retrieval_params.get("top_k", 5)],
                        "context": context_str,
                        "metadata": {
                            **metadata,
                            "total_time": total_time,
                            "model": generation_result.get("model", "unknown"),
                            "token_usage": generation_result.get("usage", {}),
                        },
                        "generation_metrics": generation_result,
                        "quality_check": generation_result.get("quality_check", {}),
                    }
                )

            return final_result

        except Exception as e:
            logger.error(f"âŒ Agentic RAGæµç¨‹å¤±è´¥: {e}")
            final_result["answer"] = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            final_result["metadata"]["error"] = str(e)
            return final_result


# å·¥å…·å‡½æ•°
async def query_agentic_rag(
    query: str, history: List[Dict] = None, top_k: int = 5
) -> Dict[str, any]:
    """å¿«é€Ÿè°ƒç”¨Agentic RAGæŸ¥è¯¢"""
    engine = AgenticRAGEngine()
    return await engine.query(query, history)


# å…¼å®¹æ—§æ¥å£
async def query_rag(query: str, top_k: int = 5) -> Dict[str, any]:
    """å…¼å®¹ä¼ ç»ŸRAGæ¥å£"""
    engine = AgenticRAGEngine()
    return await engine.query(query, top_k=top_k)


"""
Agentic RAGåè°ƒå¼•æ“ - æ ¸å¿ƒå†³ç­–ä¸æµç¨‹æ§åˆ¶
å¢å¼ºèƒ½åŠ›ï¼šAgentå†³ç­–é©±åŠ¨ã€å¤šç­–ç•¥æ£€ç´¢ã€ç»“æœè¯„ä¼°ã€è‡ªåŠ¨é‡æ£€ç´¢
"""

import time
from typing import Dict, List, Optional, Tuple
from app.services.retriever import HybridVectorRetriever
from app.services.llm_service import LLMService, AgentDecision, get_llm_service
from app.core.logger import get_logger

logger = get_logger(__name__)


class AgenticRAGEngine:
    """
    Agentic RAGåè°ƒå¼•æ“
    æ ¸å¿ƒæµç¨‹ï¼š
    1. Agentæ„å›¾åˆ†æä¸å†³ç­–
    2. åŸºäºå†³ç­–çš„å¤šç­–ç•¥æ£€ç´¢
    3. æ£€ç´¢ç»“æœè´¨é‡è¯„ä¼°
    4. æ™ºèƒ½ç”Ÿæˆä¸è´¨é‡æ ¡éªŒ
    5. è‡ªåŠ¨é‡æ£€ç´¢ï¼ˆå¯é€‰ï¼‰
    """

    def __init__(self):
        self.retriever = HybridVectorRetriever()
        self.llm_service: LLMService = get_llm_service()
        self.min_quality_score = 0.7  # æœ€ä½å›ç­”è´¨é‡é˜ˆå€¼
        logger.info("ğŸ¤– Agentic RAGå¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def _evaluate_retrieval_quality(
        self, query: str, results: List[Dict], metadata: Dict
    ) -> Dict[str, any]:
        """
        Agentæ ¸å¿ƒèƒ½åŠ›ï¼šè¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡
        æ£€æŸ¥ï¼šç›¸å…³æ€§ã€è¦†ç›–åº¦ã€æƒå¨æ€§
        """
        if not results:
            return {
                "retrieval_quality_score": 0.0,
                "suggestion": "æ— æ£€ç´¢ç»“æœï¼Œå»ºè®®æ‰©å¤§æ£€ç´¢èŒƒå›´",
                "need_reretrieval": True,
            }

        # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        avg_similarity = metadata.get("avg_similarity", 0.0)
        num_results = metadata.get("num_results", 0)
        dept_coverage = len(set([r["entity"].get("department", "") for r in results]))

        # ç»¼åˆæ£€ç´¢è´¨é‡è¯„åˆ†
        retrieval_score = (
            avg_similarity * 0.6  # ç›¸ä¼¼åº¦æƒé‡60%
            + min(1.0, num_results / 10) * 0.2  # æ•°é‡æƒé‡20%
            + min(1.0, dept_coverage / 5) * 0.2  # éƒ¨é—¨è¦†ç›–åº¦æƒé‡20%
        )

        # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ£€ç´¢
        need_reretrieval = retrieval_score < 0.6 or (  # ä½è´¨é‡æ£€ç´¢ç»“æœ
            num_results < 2 and avg_similarity < 0.7
        )  # ç»“æœå°‘ä¸”ç›¸ä¼¼åº¦ä½

        return {
            "retrieval_quality_score": retrieval_score,
            "avg_similarity": avg_similarity,
            "num_results": num_results,
            "dept_coverage": dept_coverage,
            "suggestion": "éœ€è¦é‡æ£€ç´¢" if need_reretrieval else "æ£€ç´¢ç»“æœåˆæ ¼",
            "need_reretrieval": need_reretrieval,
        }

    async def _reretrieve_with_adjusted_params(
        self, query: str, decision: AgentDecision
    ) -> Tuple[str, List[Dict], Dict]:
        """
        è‡ªåŠ¨é‡æ£€ç´¢ï¼ˆè°ƒæ•´å‚æ•°ï¼‰
        """
        logger.info(f"ğŸ”„ æ‰§è¡Œé‡æ£€ç´¢ï¼Œè°ƒæ•´æ£€ç´¢å‚æ•°...")

        # è°ƒæ•´æ£€ç´¢å‚æ•°ï¼ˆæ‰©å¤§èŒƒå›´ï¼‰
        adjusted_params = decision.retrieval_params.copy()
        adjusted_params["top_k"] = min(
            10, adjusted_params["top_k"] * 2
        )  # top_kç¿»å€ï¼ˆæœ€å¤§10ï¼‰
        adjusted_params["threshold"] = max(
            0.4, adjusted_params["threshold"] * 0.8
        )  # é˜ˆå€¼é™ä½20%ï¼ˆæœ€ä½0.4ï¼‰

        # æ‰§è¡Œé‡æ£€ç´¢
        context_str, results, metadata = self.retriever.retrieve(
            query=decision.query_rewritten or query, top_k=adjusted_params["top_k"]
        )

        # æ›´æ–°å…ƒæ•°æ®
        metadata["reretrieval"] = True
        metadata["adjusted_params"] = adjusted_params

        logger.info(f"âœ… é‡æ£€ç´¢å®Œæˆï¼Œæ–°å‚æ•°: {adjusted_params}, ç»“æœæ•°: {len(results)}")

        return context_str, results, metadata

    async def query(
        self,
        query: str,
        history: List[Dict] = None,
        stream: bool = False,
        enable_reretrieval: bool = True,
    ) -> Dict[str, any]:
        """
        Agentic RAGå®Œæ•´æŸ¥è¯¢æµç¨‹

        Returns:
            {
                "answer": str,               # ç”Ÿæˆçš„å›ç­”
                "sources": List[Dict],       # æ£€ç´¢åˆ°çš„æ¥æº
                "context": str,              # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
                "metadata": Dict[str, any],  # å…ƒæ•°æ®
                "generation_metrics": Dict,  # ç”Ÿæˆç›¸å…³æŒ‡æ ‡
                "agent_decision": Dict,      # Agentå†³ç­–ç»“æœ
                "quality_check": Dict        # è´¨é‡æ ¡éªŒç»“æœ
            }
        """
        start_time = time.time()
        final_result = {
            "query": query,
            "answer": "",
            "sources": [],
            "context": "",
            "metadata": {},
            "generation_metrics": {},
            "agent_decision": {},
            "quality_check": {},
        }

        try:
            # ========== Step 1: Agentæ„å›¾åˆ†æä¸å†³ç­– ==========
            logger.info(f"ğŸ§  Agentåˆ†ææŸ¥è¯¢æ„å›¾: {query}")
            decision: AgentDecision = await self.llm_service.analyze_query_intent(
                query, history
            )
            final_result["agent_decision"] = decision.model_dump()

            # ç›´æ¥å›ç­”ï¼ˆæ— éœ€æ£€ç´¢ï¼‰
            if decision.decision_type == "direct_answer":
                logger.info(f"ğŸ’¡ Agentå†³ç­–ï¼šç›´æ¥å›ç­”ï¼Œæ— éœ€æ£€ç´¢")
                # ç›´æ¥ç”Ÿæˆå›ç­”
                generation_result = await self.llm_service.generate_response(
                    query=query,
                    context="æ— éœ€æ£€ç´¢çš„é€šç”¨æ”¿åŠ¡é—®é¢˜",
                    history=history,
                    decision=decision,
                    stream=stream,
                )

                final_result.update(
                    {
                        "answer": generation_result["answer"],
                        "generation_metrics": generation_result,
                        "quality_check": generation_result.get("quality_check", {}),
                    }
                )
                return final_result

            # æ— æ³•å›ç­”
            if decision.decision_type == "cannot_answer":
                logger.info(f"ğŸš« Agentå†³ç­–ï¼šæ— æ³•å›ç­”è¯¥é—®é¢˜")
                final_result["answer"] = (
                    "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚è¯·ç¡®è®¤é—®é¢˜æ˜¯å¦å±äºæ³¸å·å¸‚æ”¿åŠ¡èŒƒç•´ï¼Œæˆ–é€šè¿‡æ”¿åŠ¡æœåŠ¡çƒ­çº¿12345å’¨è¯¢ã€‚"
                )
                final_result["quality_check"] = {"overall_score": 0.0}
                return final_result

            # ========== Step 2: åŸºäºAgentå†³ç­–çš„æ£€ç´¢ ==========
            logger.info(
                f"ğŸ” Agenté©±åŠ¨æ£€ç´¢ï¼Œç­–ç•¥: {decision.retrieval_strategy}, å‚æ•°: {decision.retrieval_params}"
            )

            # æ‰§è¡Œæ£€ç´¢
            context_str, results, metadata = self.retriever.retrieve(
                query=decision.query_rewritten or query,
                top_k=decision.retrieval_params.get("top_k", 5),
            )

            retrieval_time = time.time() - start_time
            metadata["retrieval_time"] = retrieval_time
            logger.info(
                f"âœ… åˆå§‹æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {retrieval_time:.2f}sï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ"
            )

            # ========== Step 3: æ£€ç´¢ç»“æœè´¨é‡è¯„ä¼° ==========
            retrieval_quality = await self._evaluate_retrieval_quality(
                query, results, metadata
            )
            metadata["retrieval_quality"] = retrieval_quality
            logger.info(
                f"ğŸ“Š æ£€ç´¢è´¨é‡è¯„åˆ†: {retrieval_quality['retrieval_quality_score']:.3f}"
            )

            # è‡ªåŠ¨é‡æ£€ç´¢ï¼ˆå¦‚æœå¼€å¯ä¸”éœ€è¦ï¼‰
            if enable_reretrieval and retrieval_quality["need_reretrieval"]:
                context_str, results, metadata = (
                    await self._reretrieve_with_adjusted_params(query, decision)
                )
                # é‡æ–°è¯„ä¼°é‡æ£€ç´¢ç»“æœ
                retrieval_quality = await self._evaluate_retrieval_quality(
                    query, results, metadata
                )

            # ========== Step 4: æ™ºèƒ½ç”Ÿæˆå›ç­” ==========
            logger.info(f"ğŸ¤– Agentå¼€å§‹ç”Ÿæˆå›ç­”...")
            generation_start = time.time()

            if stream:
                # æµå¼ç”Ÿæˆï¼ˆè¿”å›ç”Ÿæˆå™¨ï¼‰
                final_result["stream_generator"] = (
                    await self.llm_service.generate_response(
                        query=query,
                        context=context_str,
                        history=history,
                        decision=decision,
                        stream=stream,
                    )
                )
            else:
                # æ™®é€šç”Ÿæˆ
                generation_result = await self.llm_service.generate_response(
                    query=query, context=context_str, history=history, decision=decision
                )

                generation_time = time.time() - generation_start
                metadata["generation_time"] = generation_time

                # ========== Step 5: æ•´åˆæœ€ç»ˆç»“æœ ==========
                total_time = time.time() - start_time

                final_result.update(
                    {
                        "answer": generation_result["answer"],
                        "sources": results[: decision.retrieval_params.get("top_k", 5)],
                        "context": context_str,
                        "metadata": {
                            **metadata,
                            "total_time": total_time,
                            "model": generation_result.get("model", "unknown"),
                            "token_usage": generation_result.get("usage", {}),
                        },
                        "generation_metrics": generation_result,
                        "quality_check": generation_result.get("quality_check", {}),
                    }
                )

            return final_result

        except Exception as e:
            logger.error(f"âŒ Agentic RAGæµç¨‹å¤±è´¥: {e}")
            final_result["answer"] = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            final_result["metadata"]["error"] = str(e)
            return final_result


# å·¥å…·å‡½æ•°
async def query_agentic_rag(
    query: str, history: List[Dict] = None, top_k: int = 5
) -> Dict[str, any]:
    """å¿«é€Ÿè°ƒç”¨Agentic RAGæŸ¥è¯¢"""
    engine = AgenticRAGEngine()
    return await engine.query(query, history)


# å…¼å®¹æ—§æ¥å£
async def query_rag(query: str, top_k: int = 5) -> Dict[str, any]:
    """å…¼å®¹ä¼ ç»ŸRAGæ¥å£"""
    engine = AgenticRAGEngine()
    return await engine.query(query, top_k=top_k)
