"""
æ··åˆå‘é‡æ£€ç´¢å™¨
ç»“åˆå‘é‡æ£€ç´¢ + å¤šç»´åº¦é‡æ’ + ç¼“å­˜çš„å®Œæ•´å®ç°
"""

import threading
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from sentence_transformers import SentenceTransformer

from src.config.setting import settings
from src.app.infra.utils import get_device
from src.app.infra.db.milvus_db import get_milvus_client
from .base_retriever import BaseRetriever


class HybridVectorRetriever(BaseRetriever):
    """
    æ··åˆå‘é‡æ£€ç´¢å™¨

    ç»§æ‰¿è‡ª BaseRetrieverï¼Œå®ç°å…·ä½“çš„å‘é‡æ£€ç´¢é€»è¾‘
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, config: Optional[Dict[str, Any]] = None):
        """å•ä¾‹æ¨¡å¼"""
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super(HybridVectorRetriever, cls).__new__(cls)
        return cls._instance

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        if getattr(self, "_is_initialized", False):
            return

        # åˆå¹¶é…ç½®ï¼šç”¨æˆ·é…ç½®ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨ settings
        default_config = {
            "top_k": settings.vectordb.default_top_k,
            "cache_enabled": settings.retriever.enable_cache,
            "cache_ttl": settings.retriever.cache_ttl_minutes * 60,
            "max_cache_size": settings.retriever.cache_max_size,
            "min_similarity": settings.retriever.min_similarity,
        }

        if config:
            default_config.update(config)

        super().__init__(default_config)

        print("ğŸ”„ [HybridRetriever] åˆå§‹åŒ–æ··åˆç­–ç•¥æ£€ç´¢å™¨...")
        self.initialize()
        self._is_initialized = True
        print("âœ… [HybridRetriever] åˆå§‹åŒ–å®Œæˆ")

    def initialize(self) -> None:
        """
        åˆå§‹åŒ–æ ¸å¿ƒèµ„æº

        å®ç° BaseRetriever çš„æŠ½è±¡æ–¹æ³•
        """
        # 1. åŠ è½½ Embedding æ¨¡å‹
        self.device = get_device()
        print(f"ğŸ“¥ åŠ è½½ Embedding æ¨¡å‹: {settings.models.embedding_model_path} ...")
        self.embed_model = SentenceTransformer(
            str(settings.models.embedding_model_path),
            device=self.device
        )

        # 2. è¿æ¥å‘é‡æ•°æ®åº“
        print(f"ğŸ”Œ è¿æ¥ Milvus: {settings.vectordb.db_path} ...")
        self.milvus_client = get_milvus_client()
        self.collection_name = settings.vectordb.collection_name

        # 3. æ··åˆç­–ç•¥é…ç½®
        self.min_results = settings.retriever.min_results
        self.max_results = settings.retriever.max_results

        # 4. é‡æ’æƒé‡é…ç½®
        # æ³¨æ„ï¼šéƒ¨é—¨æƒå¨æ€§å·²è¢«ç§»é™¤ï¼Œæƒé‡è®¾ä¸º 0.0ï¼Œä½†ä¿ç•™é”®ä»¥å…¼å®¹ä»£ç 
        self.rerank_weights = {
            "similarity": settings.retriever.weight_similarity,
            "recency": settings.retriever.weight_recency,
            "authority": 0.0,  # å·²ç§»é™¤éƒ¨é—¨æƒå¨æ€§ï¼Œæƒé‡ä¸º0
            "length": settings.retriever.weight_length,
        }

        # 6. æ—¶é—´è¡°å‡æƒé‡
        self.recency_weights = settings.retriever.recency_weights

    def retrieve(
        self,
        query: str,
        top_k: Optional[int] = None,
        **kwargs
    ) -> Tuple[str, List[Dict[str, Any]], Dict[str, Any]]:
        """
        æ‰§è¡Œæ··åˆæ£€ç´¢

        å®ç° BaseRetriever çš„æŠ½è±¡æ–¹æ³•

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚é˜ˆå€¼è°ƒæ•´ç­‰ï¼‰

        Returns:
            (context_str, results, metadata)
        """
        start_time = datetime.now()

        # ä½¿ç”¨é…ç½®çš„ top_k
        if top_k is None:
            top_k = self.default_top_k

        # 1. æ£€æŸ¥ç¼“å­˜
        if self.cache_enabled:
            cache_key = self._get_cache_key(query, top_k, **kwargs)
            cached_result = self._check_cache(cache_key)
            if cached_result:
                context, results, metadata = cached_result
                metadata["cache_hit"] = True
                print(f"ğŸ”„ ä½¿ç”¨ç¼“å­˜ç»“æœ: {cache_key[:30]}...")
                return context, results, metadata

        try:
            # 2. å‘é‡æ£€ç´¢
            query_vec = self.embed_model.encode([query], normalize_embeddings=True)

            # æ”¾å®½æ£€ç´¢æ•°é‡ï¼Œä¸ºåç»­ç­›é€‰å’Œé‡æ’å‡†å¤‡
            search_limit = top_k * 3
            raw_results = self.milvus_client.search(
                vectors=query_vec.tolist(),
                top_k=search_limit,
                output_fields=["text", "department", "metadata"],
            )

            if not raw_results or not raw_results[0]:
                result = ("æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", [], {})
                if self.cache_enabled:
                    self._update_cache(cache_key, *result)
                return result

            # 3. è½¬æ¢ç»“æœå¹¶è®¡ç®—ç›¸ä¼¼åº¦
            processed_results = []
            for hit in raw_results[0]:
                # Milvus 2.x ä¸­ï¼Œdistance å¯¹åº”ä½™å¼¦ç›¸ä¼¼åº¦
                processed_hit = {
                    "entity": hit.get("entity", hit),
                    "distance": 1 - hit.get("distance", 0),
                    "similarity": hit.get("distance", 0),
                }
                processed_results.append(processed_hit)

            # 4. æ··åˆé˜ˆå€¼ç­›é€‰
            filtered_results = self._hybrid_threshold_filter(processed_results, query)

            # 5. æ··åˆé‡æ’
            reranked_results = self._hybrid_rerank(query, filtered_results)

            # 6. æˆªå–æœ€ç»ˆç»“æœ
            final_results = reranked_results[:min(top_k, len(reranked_results))]

            # 7. æ„å»ºä¸Šä¸‹æ–‡
            context_str = self.build_context(query, final_results)

            # 8. å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                "query": query,
                "retrieval_time": (datetime.now() - start_time).total_seconds(),
                "num_results": len(final_results),
                "num_raw_results": len(processed_results),
                "avg_similarity": (
                    np.mean([r["similarity"] for r in final_results])
                    if final_results else 0
                ),
                "threshold_applied": self.min_similarity,
                "cache_hit": False,
            }

            # 9. æ›´æ–°ç¼“å­˜
            if self.cache_enabled:
                self._update_cache(cache_key, context_str, final_results, metadata)

            return context_str, final_results, metadata

        except Exception as e:
            print(f"âš ï¸ [HybridRetriever] æ£€ç´¢å¤±è´¥: {e}")
            return f"æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}", [], {}

    def retrieve_with_details(
        self,
        query: str,
        top_k: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        è¯¦ç»†æœç´¢æ¥å£

        å®ç° BaseRetriever çš„æŠ½è±¡æ–¹æ³•

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åŒ…å«å®Œæ•´ä¿¡æ¯çš„å­—å…¸
        """
        context_str, results, metadata = self.retrieve(query, top_k, **kwargs)

        # æå–å…³é”®ä¿¡æ¯
        sources = []
        for i, hit in enumerate(results):
            entity = hit.get("entity", {})
            meta = entity.get("metadata", {})

            sources.append({
                "rank": i + 1,
                "similarity": hit.get("similarity", 0),
                "department": entity.get("department", "æœªçŸ¥éƒ¨é—¨"),
                "title": meta.get("title", "æ— æ ‡é¢˜"),
                "time": meta.get("time", "æœªçŸ¥æ—¶é—´"),
                "composite_score": hit.get("composite_score", 0),
                "features": hit.get("rerank_features", {}),
            })

        return {
            "query": query,
            "context": context_str,
            "sources": sources,
            "metadata": metadata,
            "num_sources": len(sources),
            "confidence": self.calculate_confidence(results),
        }

    # ==================== å†…éƒ¨æ–¹æ³• ====================

    def _hybrid_threshold_filter(self, results: List[Dict], query: str) -> List[Dict]:
        """
        æ··åˆé˜ˆå€¼ç­›é€‰ç­–ç•¥

        ç­–ç•¥æ­¥éª¤ï¼š
        1. ä½¿ç”¨åŸºç¡€é˜ˆå€¼ç­›é€‰
        2. å¦‚æœç»“æœå¤ªå°‘ï¼ŒåŠ¨æ€é™ä½é˜ˆå€¼
        3. ç¡®ä¿è‡³å°‘æœ‰æœ€å°ç»“æœæ•°é‡
        """
        if not results:
            return []

        # æ­¥éª¤1ï¼šåŸºç¡€é˜ˆå€¼ç­›é€‰
        threshold = self.min_similarity
        filtered = [r for r in results if r["similarity"] >= threshold]

        # æ­¥éª¤2ï¼šåˆ†æç»“æœåˆ†å¸ƒ
        similarities = [r["similarity"] for r in results[:10]]  # åªçœ‹å‰10ä¸ª
        mean_sim = np.mean(similarities) if similarities else 0

        # æ­¥éª¤3ï¼šåŠ¨æ€è°ƒæ•´
        if len(filtered) < self.min_results:
            if mean_sim < threshold:
                # å¦‚æœæ•´ä½“ç›¸ä¼¼åº¦è¾ƒä½ï¼Œé€‚å½“é™ä½é˜ˆå€¼
                adaptive_threshold = max(threshold * 0.8, mean_sim - 0.1)
                adaptive_threshold = max(0.3, adaptive_threshold)  # ä¿åº•é˜ˆå€¼

                print(f"ğŸ“Š é˜ˆå€¼åŠ¨æ€è°ƒæ•´: {threshold:.3f} â†’ {adaptive_threshold:.3f}")
                filtered = [r for r in results if r["similarity"] >= adaptive_threshold]

        # è¿”å›ç­›é€‰åçš„ç»“æœ
        return filtered if len(filtered) > 0 else results

    def _hybrid_rerank(self, query: str, results: List[Dict]) -> List[Dict]:
        """
        æ··åˆé‡æ’ç­–ç•¥

        åŸºäºå¤šä¸ªç‰¹å¾ç»¼åˆè¯„åˆ†ï¼ˆç§»é™¤éƒ¨é—¨æƒå¨æ€§ï¼Œæ‰€æœ‰éƒ¨é—¨ä¿¡æ¯å¹³ç­‰å¯¹å¾…ï¼‰ï¼š
        1. å‘é‡ç›¸ä¼¼åº¦ (60%)
        2. æ—¶æ•ˆæ€§ (30%)
        3. å†…å®¹é•¿åº¦ (10%)
        """
        if len(results) <= 1:
            return results

        current_time = datetime.now()
        features = {key: [] for key in self.rerank_weights.keys()}

        for hit in results:
            # 1. ç›¸ä¼¼åº¦ç‰¹å¾
            features["similarity"].append(hit["similarity"])

            # 2. æ—¶æ•ˆæ€§ç‰¹å¾
            time_str = hit["entity"].get("metadata", {}).get("time", "")
            recency = self._calculate_recency(time_str, current_time)
            features["recency"].append(recency)

            # 3. éƒ¨é—¨æƒå¨æ€§ç‰¹å¾ï¼ˆå·²ç§»é™¤ï¼Œè®¾ä¸º0ï¼‰
            # æ”¿åŠ¡æ•°æ®ç‰¹ç‚¹ï¼šæ‰€æœ‰æ”¿åºœéƒ¨é—¨å‘å¸ƒçš„ä¿¡æ¯éƒ½å…·æœ‰æƒå¨æ€§
            # å…¬å¹³æ€§åŸåˆ™ï¼šæ‰€æœ‰éƒ¨é—¨çš„æ”¿ç­–å’Œå›å¤éƒ½åº”è¯¥å¹³ç­‰å¯¹å¾…
            features["authority"].append(0.0)

            # 4. å†…å®¹é•¿åº¦ç‰¹å¾
            text_len = len(hit["entity"].get("text", ""))
            length_score = min(1.0, text_len / 1500)  # 1500å­—ä¸ºç†æƒ³é•¿åº¦
            features["length"].append(length_score)

        # å½’ä¸€åŒ–ç‰¹å¾
        norm_features = {}
        for key, values in features.items():
            norm_features[key] = self._normalize_features(values)

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        for i, hit in enumerate(results):
            composite_score = 0
            for key, weight in self.rerank_weights.items():
                composite_score += norm_features[key][i] * weight

            hit["composite_score"] = composite_score
            hit["rerank_features"] = {
                key: norm_features[key][i] for key in self.rerank_weights.keys()
            }

        # æŒ‰ç»¼åˆè¯„åˆ†é™åºæ’åº
        results.sort(key=lambda x: x["composite_score"], reverse=True)
        return results

    def _calculate_recency(self, time_str: str, current_time: datetime) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°"""
        if not time_str:
            return 0.5

        try:
            # å°è¯•è§£æå¸¸è§æ—¶é—´æ ¼å¼
            formats = ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%Y/%m/%d"]
            item_time = None

            for fmt in formats:
                try:
                    item_time = datetime.strptime(time_str.split()[0], fmt)
                    break
                except:
                    continue

            if item_time:
                # è®¡ç®—æ—¶é—´è¡°å‡ï¼ˆè¶Šè¿‘åˆ†æ•°è¶Šé«˜ï¼‰
                days_diff = (current_time - item_time).days
                if days_diff < 0:  # æœªæ¥æ—¶é—´
                    return 0.5
                elif days_diff <= 7:  # ä¸€å‘¨å†…
                    return 1.0
                elif days_diff <= 30:  # ä¸€æœˆå†…
                    return 0.9
                elif days_diff <= 90:  # ä¸‰æœˆå†…
                    return 0.7
                elif days_diff <= 365:  # ä¸€å¹´å†…
                    return 0.5
                else:  # è¶…è¿‡ä¸€å¹´
                    return 0.3
        except:
            pass

        return 0.5  # é»˜è®¤å€¼

    def _normalize_features(self, values: List[float]) -> List[float]:
        """å½’ä¸€åŒ–ç‰¹å¾å€¼åˆ°0-1èŒƒå›´"""
        if not values:
            return values

        min_val, max_val = min(values), max(values)

        if max_val - min_val < 1e-6:  # é¿å…é™¤ä»¥0
            return [0.5] * len(values)

        return [(v - min_val) / (max_val - min_val) for v in values]

    # ==================== é™æ€å·¥å‚æ–¹æ³• ====================

    @classmethod
    def from_config(cls, config: Dict[str, Any]) -> "HybridVectorRetriever":
        """
        ä»é…ç½®åˆ›å»ºå®ä¾‹

        Args:
            config: é…ç½®å­—å…¸

        Returns:
            HybridVectorRetriever å®ä¾‹
        """
        return cls(config=config)

    @classmethod
    def from_settings(cls) -> "HybridVectorRetriever":
        """
        ä»é¡¹ç›®é…ç½®åˆ›å»ºå®ä¾‹

        Returns:
            HybridVectorRetriever å®ä¾‹
        """
        return cls()