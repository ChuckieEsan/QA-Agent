import threading
import numpy as np
from typing import List, Dict, Any, Tuple
from datetime import datetime, timedelta
from pymilvus import MilvusClient
from sentence_transformers import SentenceTransformer
from src.config.setting import settings
from src.app.infra.utils import get_device


class HybridVectorRetriever:
    """
    çº¯æ··åˆç­–ç•¥å‘é‡æ£€ç´¢å™¨
    ç»“åˆå›ºå®šé˜ˆå€¼+åŠ¨æ€è°ƒæ•´+é‡æ’çš„å®Œæ•´æµç¨‹
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super(HybridVectorRetriever, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if getattr(self, "_is_initialized", False):
            return

        print("ğŸ”„ [HybridRetriever] åˆå§‹åŒ–æ··åˆç­–ç•¥æ£€ç´¢å™¨...")
        self._init_resources()
        self._is_initialized = True
        print("âœ… [HybridRetriever] åˆå§‹åŒ–å®Œæˆ")

    def _init_resources(self):
        """åˆå§‹åŒ–æ ¸å¿ƒèµ„æº"""
        # 1. åŠ è½½Embeddingæ¨¡å‹
        self.device = get_device()
        print(f"ğŸ“¥ åŠ è½½Embeddingæ¨¡å‹: {settings.models.embedding_model_path} ...")
        self.embed_model = SentenceTransformer(
            str(settings.models.embedding_model_path), device=self.device
        )

        # 2. è¿æ¥å‘é‡æ•°æ®åº“
        print(f"ğŸ”Œ è¿æ¥Milvus: {settings.vectordb.db_path} ...")
        self.client = MilvusClient(str(settings.vectordb.db_path))
        self.collection = settings.vectordb.collection_name

        # 3. æ··åˆç­–ç•¥é…ç½®
        self.base_threshold = settings.retriever.base_threshold
        self.min_results = settings.retriever.min_results
        self.max_results = settings.retriever.max_results

        # 4. é‡æ’æƒé‡é…ç½®
        self.rerank_weights = {
            "similarity": settings.retriever.weight_similarity,
            "recency": settings.retriever.weight_recency,
            "authority": settings.retriever.weight_authority,
            "length": settings.retriever.weight_length,
        }

        # TODO: 5. éƒ¨é—¨æƒå¨æ€§æ˜ å°„
        self.dept_authority = settings.retriever.department_authority

        # TODO: 6. ç¼“å­˜. åç»­å¯ä»¥æ”¹æˆ Redis
        self.cache = {}
        self.cache_ttl = timedelta(minutes=5)

    def retrieve(self, query: str, top_k: int = None) -> Tuple[str, List[Dict], Dict]:
        """
        æ··åˆç­–ç•¥æ£€ç´¢ä¸»å‡½æ•°

        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡ï¼ˆNoneæ—¶ä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰

        è¿”å›:
            (context_str, results, metadata)
        """
        start_time = datetime.now()

        if top_k is None:
            top_k = min(self.max_results, max(self.min_results, 5))

        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"{query}_{top_k}"
        if cache_key in self.cache:
            cache_entry = self.cache[cache_key]
            if datetime.now() - cache_entry["timestamp"] < self.cache_ttl:
                print(f"ğŸ”„ ä½¿ç”¨ç¼“å­˜ç»“æœ: {cache_key[:30]}...")
                return (
                    cache_entry["context"],
                    cache_entry["results"],
                    cache_entry["metadata"],
                )

        try:
            # 2. å‘é‡æ£€ç´¢
            query_vec = self.embed_model.encode([query], normalize_embeddings=True)

            # æ”¾å®½æ£€ç´¢æ•°é‡ï¼Œä¸ºåç»­ç­›é€‰å’Œé‡æ’å‡†å¤‡
            search_limit = top_k * 3
            raw_results = self.client.search(
                collection_name=self.collection,
                data=query_vec,
                limit=search_limit,
                output_fields=["text", "department", "metadata"],
            )

            if not raw_results or not raw_results[0]:
                result = ("æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", [], {})
                self._update_cache(cache_key, result, start_time)
                return result

            # 3. è½¬æ¢ç»“æœå¹¶è®¡ç®—ç›¸ä¼¼åº¦
            processed_results = []
            for hit in raw_results[0]:
                # ç‰¹åˆ«æ³¨æ„, åœ¨ Milvus 2.x ç‰ˆæœ¬ä¸­, distance å¯¹åº”çš„å°±æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦
                processed_hit = {
                    "entity": hit["entity"],
                    "distance": 1 - hit["distance"],
                    "similarity": hit["distance"],
                }
                processed_results.append(processed_hit)

            # 4. æ··åˆé˜ˆå€¼ç­›é€‰
            filtered_results = self._hybrid_threshold_filter(processed_results, query)

            # 5. æ··åˆé‡æ’
            reranked_results = self._hybrid_rerank(query, filtered_results)

            # 6. æˆªå–æœ€ç»ˆç»“æœ
            final_results = reranked_results[: min(top_k, len(reranked_results))]

            # 7. æ„å»ºä¸Šä¸‹æ–‡
            context_str = self._build_context(query, final_results)

            # 8. å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                "query": query,
                "retrieval_time": (datetime.now() - start_time).total_seconds(),
                "num_results": len(final_results),
                "num_raw_results": len(processed_results),
                "avg_similarity": (
                    np.mean([r["similarity"] for r in final_results])
                    if final_results
                    else 0
                ),
                "threshold_applied": self.base_threshold,
                "cache_hit": False,
            }

            # 9. æ›´æ–°ç¼“å­˜
            result = (context_str, final_results, metadata)
            self._update_cache(cache_key, result, start_time)

            return result

        except Exception as e:
            print(f"âš ï¸ [HybridRetriever] æ£€ç´¢å¤±è´¥: {e}")
            return (f"æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}", [], {})

    def _hybrid_threshold_filter(self, results: List[Dict], query: str) -> List[Dict]:
        """
        æ··åˆé˜ˆå€¼ç­›é€‰ç­–ç•¥

        ç­–ç•¥æ­¥éª¤ï¼š
        1. ä½¿ç”¨åŸºç¡€é˜ˆå€¼ç­›é€‰
        2. å¦‚æœç»“æœå¤ªå°‘ï¼ŒåŠ¨æ€é™ä½é˜ˆå€¼
        3. ç¡®ä¿è‡³å°‘æœ‰æœ€å°ç»“æœæ•°é‡
        """
        if not results:
            return []

        # æ­¥éª¤1ï¼šåŸºç¡€é˜ˆå€¼ç­›é€‰
        base_threshold = self.base_threshold
        filtered = [r for r in results if r["similarity"] >= base_threshold]

        # æ­¥éª¤2ï¼šåˆ†æç»“æœåˆ†å¸ƒ
        similarities = [r["similarity"] for r in results[:10]]  # åªçœ‹å‰10ä¸ª
        mean_sim = np.mean(similarities) if similarities else 0

        # æ­¥éª¤3ï¼šåŠ¨æ€è°ƒæ•´
        if len(filtered) < self.min_results:
            if mean_sim < base_threshold:
                # å¦‚æœæ•´ä½“ç›¸ä¼¼åº¦è¾ƒä½ï¼Œé€‚å½“é™ä½é˜ˆå€¼
                adaptive_threshold = max(base_threshold * 0.8, mean_sim - 0.1)
                adaptive_threshold = max(0.3, adaptive_threshold)  # ä¿åº•é˜ˆå€¼

                print(
                    f"ğŸ“Š é˜ˆå€¼åŠ¨æ€è°ƒæ•´: {base_threshold:.3f} â†’ {adaptive_threshold:.3f}"
                )
                filtered = [r for r in results if r["similarity"] >= adaptive_threshold]

        # æ­¥éª¤4ï¼šä¿åº•æœºåˆ¶
        if len(filtered) < self.min_results:
            # è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„å‰å‡ ä¸ªç»“æœï¼Œä½†æ ‡è®°ä¸ºä½ç½®ä¿¡åº¦
            sorted_results = sorted(
                results, key=lambda x: x["similarity"], reverse=True
            )
            filtered = sorted_results[: self.min_results]
            for r in filtered:
                r["low_confidence"] = True

        # TODO: æš‚æ—¶ä¸è€ƒè™‘è¿‡æ»¤
        # return filtered
        return results

    def _hybrid_rerank(self, query: str, results: List[Dict]) -> List[Dict]:
        """
        æ··åˆé‡æ’ç­–ç•¥

        åŸºäºå¤šä¸ªç‰¹å¾ç»¼åˆè¯„åˆ†ï¼š
        1. å‘é‡ç›¸ä¼¼åº¦
        2. æ—¶æ•ˆæ€§
        3. éƒ¨é—¨æƒå¨æ€§
        4. å†…å®¹è´¨é‡
        """
        if len(results) <= 1:
            return results

        current_time = datetime.now()
        features = {key: [] for key in self.rerank_weights.keys()}

        for hit in results:
            # 1. ç›¸ä¼¼åº¦ç‰¹å¾
            features["similarity"].append(hit["similarity"])

            # 2. æ—¶æ•ˆæ€§ç‰¹å¾
            time_str = hit["entity"].get("metadata", {}).get("time", "")
            recency = self._calculate_recency(time_str, current_time)
            features["recency"].append(recency)

            # 3. éƒ¨é—¨æƒå¨æ€§ç‰¹å¾
            dept = hit["entity"].get("department", "")
            authority = self.dept_authority.get(dept, self.dept_authority["default"])
            features["authority"].append(authority)

            # TODO: 4. å†…å®¹é•¿åº¦ç‰¹å¾. éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–
            text_len = len(hit["entity"].get("text", ""))
            length_score = min(1.0, text_len / 1500)  # 1500å­—ä¸ºç†æƒ³é•¿åº¦
            features["length"].append(length_score)

        # å½’ä¸€åŒ–ç‰¹å¾
        norm_features = {}
        for key, values in features.items():
            norm_features[key] = self._normalize_features(values)

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        for i, hit in enumerate(results):
            composite_score = 0
            for key, weight in self.rerank_weights.items():
                composite_score += norm_features[key][i] * weight

            hit["composite_score"] = composite_score
            hit["rerank_features"] = {
                key: norm_features[key][i] for key in self.rerank_weights.keys()
            }

        # TODO: æŒ‰ç»¼åˆè¯„åˆ†é‡æ’ (æš‚æ—¶ä¸è€ƒè™‘é‡æ’)
        # return sorted(results, key=lambda x: x["composite_score"], reverse=True)
        return results

    def _calculate_recency(self, time_str: str, current_time: datetime) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°"""
        if not time_str:
            return 0.5

        try:
            # å°è¯•è§£æå¸¸è§æ—¶é—´æ ¼å¼
            formats = ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%Y/%m/%d"]
            item_time = None

            for fmt in formats:
                try:
                    item_time = datetime.strptime(time_str.split()[0], fmt)
                    break
                except:
                    continue

            if item_time:
                # è®¡ç®—æ—¶é—´è¡°å‡ï¼ˆè¶Šè¿‘åˆ†æ•°è¶Šé«˜ï¼‰
                days_diff = (current_time - item_time).days
                if days_diff < 0:  # æœªæ¥æ—¶é—´
                    return 0.5
                elif days_diff <= 7:  # ä¸€å‘¨å†…
                    return 1.0
                elif days_diff <= 30:  # ä¸€æœˆå†…
                    return 0.9
                elif days_diff <= 90:  # ä¸‰æœˆå†…
                    return 0.7
                elif days_diff <= 365:  # ä¸€å¹´å†…
                    return 0.5
                else:  # è¶…è¿‡ä¸€å¹´
                    return 0.3
        except:
            pass

        return 0.5  # é»˜è®¤å€¼

    def _normalize_features(self, values: List[float]) -> List[float]:
        """å½’ä¸€åŒ–ç‰¹å¾å€¼åˆ°0-1èŒƒå›´"""
        if not values:
            return values

        min_val, max_val = min(values), max(values)

        if max_val - min_val < 1e-6:  # é¿å…é™¤ä»¥0
            return [0.5] * len(values)

        return [(v - min_val) / (max_val - min_val) for v in values]

    def _build_context(self, query: str, results: List[Dict]) -> str:
        """æ„å»ºRAGä¸Šä¸‹æ–‡"""
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³æ¡ˆä¾‹ã€‚"

        context_parts = [f"ç”¨æˆ·æŸ¥è¯¢ï¼š{query}", f"æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ¡ˆä¾‹ï¼š\n"]

        for i, hit in enumerate(results):
            similarity = hit["similarity"]
            confidence = (
                "é«˜" if similarity > 0.7 else ("ä¸­" if similarity > 0.5 else "ä½")
            )

            # å¦‚æœæœ‰é‡æ’è¯„åˆ†ï¼Œæ˜¾ç¤ºç»¼åˆè¯„åˆ†
            if "composite_score" in hit:
                composite_score = hit["composite_score"]
                score_info = f"(ç›¸ä¼¼åº¦: {similarity:.1%}, ç»¼åˆè¯„åˆ†: {composite_score:.3f}, ç½®ä¿¡åº¦: {confidence})"
            else:
                score_info = f"(ç›¸ä¼¼åº¦: {similarity:.1%}, ç½®ä¿¡åº¦: {confidence})"

            # ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„RAGä¸Šä¸‹æ–‡
            rag_text = hit["entity"].get("text", "")

            context_parts.append(f"\n--- æ¡ˆä¾‹ {i+1} {score_info} ---")
            context_parts.append(rag_text)

        # æ·»åŠ å›ç­”æŒ‡å¯¼
        context_parts.append("\n--- å›ç­”æŒ‡å¯¼ ---")
        context_parts.append("è¯·åŸºäºä»¥ä¸Šæ¡ˆä¾‹ä¿¡æ¯ï¼Œå‡†ç¡®ã€ä¸“ä¸šåœ°å›åº”ç”¨æˆ·æŸ¥è¯¢ã€‚")
        context_parts.append("å¦‚æœæ¡ˆä¾‹ä¸æŸ¥è¯¢ä¸å®Œå…¨åŒ¹é…ï¼Œè¯·è¯´æ˜å·®å¼‚å¹¶æä¾›æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚")
        context_parts.append("å¼•ç”¨å…·ä½“æ¡ˆä¾‹æ—¶ï¼Œè¯·æ³¨æ˜æ¥æºéƒ¨é—¨å’Œæ—¶é—´ã€‚")

        return "\n".join(context_parts)

    def _update_cache(self, cache_key: str, result: Tuple, timestamp: datetime):
        """æ›´æ–°ç¼“å­˜"""
        context_str, results, metadata = result

        # åªç¼“å­˜æˆåŠŸçš„æŸ¥è¯¢
        if results:
            self.cache[cache_key] = {
                "context": context_str,
                "results": results,
                "metadata": {**metadata, "cache_hit": True},
                "timestamp": timestamp,
            }

            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.cache) > 100:
                # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]

    def search_with_details(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """
        è¯¦ç»†æœç´¢æ¥å£ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰

        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡

        è¿”å›:
            åŒ…å«å®Œæ•´ä¿¡æ¯çš„å­—å…¸
        """
        context_str, results, metadata = self.retrieve(query, top_k)

        # æå–å…³é”®ä¿¡æ¯
        sources = []
        for i, hit in enumerate(results):
            entity = hit["entity"]
            meta = entity.get("metadata", {})

            sources.append(
                {
                    "rank": i + 1,
                    "similarity": hit["similarity"],
                    "department": entity.get("department", "æœªçŸ¥éƒ¨é—¨"),
                    "title": meta.get("title", "æ— æ ‡é¢˜"),
                    "time": meta.get("time", "æœªçŸ¥æ—¶é—´"),
                    "composite_score": hit.get("composite_score", 0),
                }
            )

        return {
            "query": query,
            "context": context_str,
            "sources": sources,
            "metadata": metadata,
            "num_sources": len(sources),
            "confidence": self._calculate_confidence(results),
        }

    def _calculate_confidence(self, results: List[Dict]) -> float:
        """è®¡ç®—æ£€ç´¢ç½®ä¿¡åº¦"""
        if not results:
            return 0.0

        # åŸºäºç›¸ä¼¼åº¦å’Œæ•°é‡è®¡ç®—ç½®ä¿¡åº¦
        similarities = [r["similarity"] for r in results]
        avg_similarity = np.mean(similarities)

        # æ•°é‡å› å­ï¼šç»“æœè¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜ï¼ˆä½†è¾¹é™…é€’å‡ï¼‰
        num_factor = 1 - 0.5 ** len(results)

        # ç»¼åˆç½®ä¿¡åº¦
        confidence = avg_similarity * num_factor
        return min(1.0, confidence)

    # TODO: ç¼“å­˜ç®¡ç†æ¥å£
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        print("ğŸ§¹ ç¼“å­˜å·²æ¸…ç©º")


# å·¥å…·å‡½æ•°
def retrieve_with_details(query: str, top_k: int = 5) -> Dict[str, Any]:
    """
    è·å–RAGä¸Šä¸‹æ–‡åŠè¯¦ç»†ä¿¡æ¯
    """
    retriever = HybridVectorRetriever()
    return retriever.search_with_details(query, top_k)


def get_retriever_instance() -> HybridVectorRetriever:
    """
    è·å–æ£€ç´¢å™¨å•ä¾‹å®ä¾‹
    """
    return HybridVectorRetriever()
