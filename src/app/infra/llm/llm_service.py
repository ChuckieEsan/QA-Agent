"""
LLMç”ŸæˆæœåŠ¡ - Agentic RAG å¢å¼ºç‰ˆ
è´Ÿè´£ï¼šæ„å›¾åˆ†æã€Promptä¼˜åŒ–ã€å›ç­”æ ¡éªŒã€å†³ç­–ç”Ÿæˆ
"""

import asyncio
import json
from typing import Dict, List, Optional, AsyncGenerator, Literal
from datetime import datetime

import dashscope
from dashscope import Generation
from src.config.setting import settings
from src.app.infra.utils.logger import get_logger
from src.app.infra.llm.schema import Message, ContentItem, SYSTEM, USER, ASSISTANT, BaseModelCompatibleDict
from src.app.infra.llm.base_llm_service import BaseLLMService

logger = get_logger(__name__)

# Agent å†³ç­–ç±»å‹
AgentDecisionType = Literal[
    "direct_answer",    # æ— éœ€æ£€ç´¢ï¼Œç›´æ¥å›ç­”
    "need_retrieval",   # éœ€è¦æ£€ç´¢åå›ç­”
    "multi_retrieval",  # éœ€è¦å¤šç­–ç•¥æ£€ç´¢
    "cannot_answer"     # æ— æ³•å›ç­”
]

# æ£€ç´¢ç­–ç•¥ç±»å‹
RetrievalStrategy = Literal[
    "hybrid",           # æ··åˆå‘é‡æ£€ç´¢ï¼ˆé»˜è®¤ï¼‰
    "keyword",          # å…³é”®è¯æ£€ç´¢
    "semantic_only",    # çº¯è¯­ä¹‰æ£€ç´¢
    "cross_dept"        # è·¨éƒ¨é—¨æ£€ç´¢
]

class AgentDecision(BaseModelCompatibleDict):
    """Agent å†³ç­–ç»“æœæ¨¡å‹"""
    decision_type: AgentDecisionType
    retrieval_strategy: Optional[RetrievalStrategy] = None
    retrieval_params: Optional[Dict] = None  # top_k/threshold ç­‰å‚æ•°
    query_rewritten: Optional[str] = None    # é‡å†™åçš„æŸ¥è¯¢
    intent: str = ""                         # æŸ¥è¯¢æ„å›¾
    confidence: float = 0.0                  # å†³ç­–ç½®ä¿¡åº¦

class LLMService(BaseLLMService):
    """
    LLMç”ŸæˆæœåŠ¡ï¼ˆAgentic RAG å¢å¼ºç‰ˆï¼‰
    æ–°å¢èƒ½åŠ›ï¼šæ„å›¾åˆ†æã€æ£€ç´¢å†³ç­–ã€Promptä¼˜åŒ–ã€å›ç­”æ ¡éªŒ
    """

    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(LLMService, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if getattr(self, "_is_initialized", False):
            return

        logger.info("ğŸ”„ åˆå§‹åŒ–Agentic LLMç”ŸæˆæœåŠ¡...")

        # é…ç½®APIå¯†é’¥
        dashscope.api_key = settings.llm.api_key

        # æ¨¡å‹é…ç½®
        self.model_name = settings.llm.model_name
        self.temperature = settings.llm.temperature
        self.max_tokens = settings.llm.max_tokens
        self.top_p = settings.llm.top_p

        # ç³»ç»ŸPromptï¼ˆAgentic RAG å¢å¼ºç‰ˆï¼‰
        self.system_prompt = self._build_system_prompt()
        self.agent_decision_prompt = self._build_agent_decision_prompt()

        # ç¼“å­˜æœ€è¿‘å¯¹è¯å†å²
        self.conversation_cache = {}

        self._is_initialized = True
        logger.info(f"âœ… Agentic LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")

    def _build_system_prompt(self) -> str:
        """æ„å»ºAgentic RAG ä¸“ç”¨ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€åå…·å¤‡æ™ºèƒ½å†³ç­–èƒ½åŠ›çš„æ”¿åŠ¡é—®ç­”Agentï¼Œä¸“é—¨å›ç­”æ³¸å·å¸‚ç›¸å…³çš„æ”¿ç­–å’¨è¯¢å’Œæ°‘ç”Ÿé—®é¢˜ã€‚

# æ ¸å¿ƒèƒ½åŠ›ï¼š
1. **æ„å›¾ç†è§£**ï¼šç²¾å‡†è¯†åˆ«ç”¨æˆ·æŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œä¿¡æ¯éœ€æ±‚
2. **æ£€ç´¢å¢å¼º**ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ¡ˆä¾‹ä¿¡æ¯ï¼Œå‡†ç¡®ã€å®Œæ•´åœ°å›ç­”é—®é¢˜
3. **æ¥æºæº¯æº**ï¼šå¿…é¡»å¼•ç”¨æ¡ˆä¾‹ä¸­çš„å…·ä½“ä¿¡æ¯ï¼Œå¹¶æ³¨æ˜æ¥æºéƒ¨é—¨å’Œæ—¶é—´
4. **è´¨é‡æ ¡éªŒ**ï¼šç¡®ä¿å›ç­”å‡†ç¡®ã€åˆè§„ã€ç¬¦åˆæ”¿åŠ¡æ²Ÿé€šè§„èŒƒ
5. **è‡ªæˆ‘ä¿®æ­£**ï¼šå¦‚æœæ£€ç´¢ä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·å¹¶æä¾›æ›¿ä»£å’¨è¯¢é€”å¾„

# å›ç­”è§„èŒƒï¼š
1. ç»“æ„åŒ–è¾“å‡ºï¼šå¤æ‚é—®é¢˜åˆ†ç‚¹è¯´æ˜ï¼Œå…³é”®ä¿¡æ¯åŠ ç²—
2. æ¥æºæ ‡æ³¨ï¼šæ ¼å¼ä¸ºã€æ¥æºï¼šXXéƒ¨é—¨ | æ—¶é—´ï¼šYYYY-MM-DDã€‘
3. æ—¶æ•ˆæ€§è¯´æ˜ï¼šæ³¨æ˜æ”¿ç­–çš„æœ‰æ•ˆæ—¶é—´èŒƒå›´
4. å…œåº•è¯´æ˜ï¼šä¿¡æ¯ä¸è¶³æ—¶ï¼Œæä¾›ç›¸å…³éƒ¨é—¨è”ç³»æ–¹å¼

# ç¦æ­¢è¡Œä¸ºï¼š
- ä¸ç¼–é€ æœªåœ¨æ¡ˆä¾‹ä¸­å‡ºç°çš„ä¿¡æ¯
- ä¸æ³„éœ²ä¸ªäººéšç§æˆ–æ•æ„Ÿä¿¡æ¯
- ä¸å›ç­”ä¸æ³¸å·å¸‚æ— å…³çš„é—®é¢˜"""

    def _build_agent_decision_prompt(self) -> str:
        """æ„å»ºAgentå†³ç­–ä¸“ç”¨Promptï¼ˆæ£€ç´¢å‰æ„å›¾åˆ†æï¼‰"""
        return """ä½ æ˜¯ä¸€åRAG Agentå†³ç­–åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶ç»™å‡ºæ£€ç´¢å†³ç­–ã€‚

# å†³ç­–ä»»åŠ¡ï¼š
1. åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾
2. åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢çŸ¥è¯†åº“
3. é€‰æ‹©æœ€ä¼˜æ£€ç´¢ç­–ç•¥
4. è°ƒæ•´æ£€ç´¢å‚æ•°ï¼ˆå¦‚top_kã€é˜ˆå€¼ï¼‰
5. å¿…è¦æ—¶é‡å†™æŸ¥è¯¢è¯­å¥

# å†³ç­–è§„åˆ™ï¼š
- direct_answerï¼šé€šç”¨æ”¿åŠ¡å¸¸è¯†ã€æ— éœ€å…·ä½“æ¡ˆä¾‹æ”¯æ’‘çš„é—®é¢˜ï¼ˆå¦‚"å¦‚ä½•åŠç†èº«ä»½è¯"çš„é€šç”¨æµç¨‹ï¼‰
- need_retrievalï¼šéœ€è¦å…·ä½“æ”¿ç­–/æ¡ˆä¾‹æ”¯æ’‘çš„é—®é¢˜ï¼ˆå¦‚"2024å¹´æ³¸å·é›¨éœ²è®¡åˆ’è¡¥è´´æ ‡å‡†"ï¼‰
- multi_retrievalï¼šè·¨éƒ¨é—¨/å¤šæ”¿ç­–çš„å¤æ‚é—®é¢˜ï¼ˆå¦‚"æ³¸å·å°å¾®ä¼ä¸šç¨æ”¶ä¼˜æƒ +ç¤¾ä¿è¡¥è´´"ï¼‰
- cannot_answerï¼šéæ³¸å·å¸‚æ”¿åŠ¡é—®é¢˜/æ— æ„ä¹‰é—®é¢˜/æ•æ„Ÿé—®é¢˜

# æ£€ç´¢ç­–ç•¥é€‰æ‹©ï¼š
- hybridï¼šé»˜è®¤ç­–ç•¥ï¼Œæ··åˆè¯­ä¹‰+å…³é”®è¯æ£€ç´¢
- keywordï¼šå¼ºå…³é”®è¯ç‰¹å¾çš„é—®é¢˜ï¼ˆå¦‚"2024å¹´æ³¸å·åŒ»ä¿ç¼´è´¹æ ‡å‡†"ï¼‰
- semantic_onlyï¼šè¯­ä¹‰æ¨¡ç³Š/å¤šä¹‰è¯é—®é¢˜ï¼ˆå¦‚"æ³¸å·åˆ›ä¸šæ‰¶æŒæ”¿ç­–"ï¼‰
- cross_deptï¼šè·¨éƒ¨é—¨é—®é¢˜ï¼ˆå¦‚"æ³¸å·ä½æˆ¿è¡¥è´´+å…¬ç§¯é‡‘æ”¿ç­–"ï¼‰

# è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
    "decision_type": "direct_answer|need_retrieval|multi_retrieval|cannot_answer",
    "retrieval_strategy": "hybrid|keyword|semantic_only|cross_dept",
    "retrieval_params": {"top_k": 5-10, "threshold": 0.5-0.8},
    "query_rewritten": "é‡å†™åçš„æŸ¥è¯¢è¯­å¥ï¼ˆå¯é€‰ï¼‰",
    "intent": "æ ¸å¿ƒæ„å›¾æè¿°",
    "confidence": 0.0-1.0
}

# æ³¨æ„ï¼š
- retrieval_strategy/cross_dept ä»…åœ¨decision_typeä¸ºneed_retrieval/multi_retrievalæ—¶å¿…å¡«
- retrieval_paramséœ€æ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´ï¼ˆå¤æ‚é—®é¢˜top_k=8-10ï¼Œç®€å•é—®é¢˜=3-5ï¼‰
- query_rewrittenéœ€æ›´ç²¾å‡†è¡¨è¾¾æ ¸å¿ƒæ„å›¾ï¼ˆå¦‚åŸé—®é¢˜"é›¨éœ²è®¡åˆ’å¤šå°‘é’±"â†’"2024å¹´æ³¸å·å¸‚é›¨éœ²è®¡åˆ’è¡¥è´´é‡‘é¢æ ‡å‡†"ï¼‰"""

    async def analyze_query_intent(self, query: str, history: List[Dict] = None) -> AgentDecision:
        """
        Agentæ ¸å¿ƒèƒ½åŠ›ï¼šåˆ†ææŸ¥è¯¢æ„å›¾å¹¶ç”Ÿæˆæ£€ç´¢å†³ç­–
        """
        # æ„å»ºå†³ç­–Prompt
        prompt_parts = [
            self.agent_decision_prompt,
            "\n# ç”¨æˆ·æŸ¥è¯¢ï¼š",
            query,
        ]

        # æ·»åŠ å¯¹è¯å†å²
        if history and len(history) > 0:
            prompt_parts.append("\n# å¯¹è¯å†å²ï¼š")
            for turn in history[-3:]:
                role = "ç”¨æˆ·" if turn["role"] == "user" else "åŠ©æ‰‹"
                prompt_parts.append(f"{role}ï¼š{turn['content']}")

        prompt = "\n".join(prompt_parts)

        try:
            # è°ƒç”¨LLMç”Ÿæˆå†³ç­–
            response = Generation.call(
                model=self.model_name,
                prompt=prompt,
                temperature=0.1,  # å†³ç­–é˜¶æ®µä½éšæœºæ€§
                max_tokens=500,
                top_p=0.9,
                result_format='text'
            )

            if response.status_code == 200:
                decision_str = response.output.choices[0].message.content
                # è§£æJSONå†³ç­–
                decision_data = json.loads(decision_str)
                return AgentDecision(**decision_data)
            else:
                raise Exception(f"å†³ç­–ç”Ÿæˆå¤±è´¥: {response.code} - {response.message}")

        except Exception as e:
            logger.error(f"âŒ Agentå†³ç­–å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å†³ç­–ï¼ˆå…œåº•ï¼‰
            return AgentDecision(
                decision_type="need_retrieval",
                retrieval_strategy="hybrid",
                retrieval_params={"top_k": 5, "threshold": 0.6},
                query_rewritten=query,
                intent=f"æ— æ³•è§£ææ„å›¾ï¼š{str(e)}",
                confidence=0.5
            )

    def build_agent_rag_prompt(self, query: str, context: str, decision: AgentDecision, history: List[Dict] = None) -> str:
        """
        æ„å»ºAgentic RAGä¸“ç”¨Promptï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        ç»“åˆAgentå†³ç­–ç»“æœï¼ŒåŠ¨æ€ä¼˜åŒ–Prompt
        """
        prompt_parts = []

        # 1. ç³»ç»ŸæŒ‡ä»¤ï¼ˆå¢å¼ºç‰ˆï¼‰
        prompt_parts.append(f"ç³»ç»ŸæŒ‡ä»¤ï¼š{self.system_prompt}")
        prompt_parts.append("")

        # 2. Agentå†³ç­–ä¿¡æ¯
        prompt_parts.append(f"## Agentå†³ç­–ä¿¡æ¯")
        prompt_parts.append(f"æŸ¥è¯¢æ„å›¾ï¼š{decision.intent}")
        prompt_parts.append(f"æ£€ç´¢ç­–ç•¥ï¼š{decision.retrieval_strategy}")
        prompt_parts.append("")

        # 3. å¯¹è¯å†å²
        if history and len(history) > 0:
            prompt_parts.append("## å¯¹è¯å†å²")
            for i, turn in enumerate(history[-3:]):
                role = "ç”¨æˆ·" if turn["role"] == "user" else "åŠ©æ‰‹"
                prompt_parts.append(f"{role}ï¼š{turn['content']}")
            prompt_parts.append("")

        # 4. æ£€ç´¢ä¸Šä¸‹æ–‡ï¼ˆå¢å¼ºæ ‡æ³¨ï¼‰
        prompt_parts.append("## æ£€ç´¢åˆ°çš„æƒå¨æ¡ˆä¾‹ä¿¡æ¯")
        prompt_parts.append(context)
        prompt_parts.append("")

        # 5. ä¼˜åŒ–åçš„æŸ¥è¯¢
        prompt_parts.append("## ç”¨æˆ·æ ¸å¿ƒé—®é¢˜")
        prompt_parts.append(decision.query_rewritten or query)
        prompt_parts.append("")

        # 6. åŠ¨æ€å›ç­”è¦æ±‚ï¼ˆåŸºäºå†³ç­–ç±»å‹ï¼‰
        prompt_parts.append("## å›ç­”è¦æ±‚")
        if decision.decision_type == "multi_retrieval":
            prompt_parts.append("1. åˆ†éƒ¨é—¨/åˆ†æ”¿ç­–ç»´åº¦å›ç­”")
            prompt_parts.append("2. æ˜ç¡®å„ç»´åº¦ä¿¡æ¯çš„æ¥æºå’Œæ—¶é—´")
            prompt_parts.append("3. æ€»ç»“å„ç»´åº¦ä¿¡æ¯çš„å…³è”æ€§")
        else:
            prompt_parts.append("1. ç²¾å‡†å¼•ç”¨æ¡ˆä¾‹ä¸­çš„å…·ä½“æ•°æ®å’Œæ”¿ç­–æ¡æ¬¾")
            prompt_parts.append("2. æŒ‰ã€æ¥æºï¼šXXéƒ¨é—¨ | æ—¶é—´ï¼šYYYY-MM-DDã€‘æ ¼å¼æ ‡æ³¨æ¥æº")
            prompt_parts.append("3. è¯­è¨€ç®€æ´ã€ä¸“ä¸šï¼Œç¬¦åˆæ”¿åŠ¡æ²Ÿé€šè§„èŒƒ")

        return "\n".join(prompt_parts)

    async def validate_answer_quality(self, answer: str, query: str, context: str) -> Dict[str, any]:
        """
        Agentæ ¸å¿ƒèƒ½åŠ›ï¼šå›ç­”è´¨é‡æ ¡éªŒ
        æ£€æŸ¥ï¼šç›¸å…³æ€§ã€å‡†ç¡®æ€§ã€æ¥æºæ ‡æ³¨ã€åˆè§„æ€§
        """
        validate_prompt = f"""
        ä½ æ˜¯å›ç­”è´¨é‡æ ¡éªŒAgentï¼Œè¯·æ ¡éªŒä»¥ä¸‹å›ç­”æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼š

        ## æ ¡éªŒæ ‡å‡†
        1. ç›¸å…³æ€§ï¼šå›ç­”æ˜¯å¦ä¸ç”¨æˆ·æŸ¥è¯¢({query})ç›´æ¥ç›¸å…³
        2. å‡†ç¡®æ€§ï¼šæ˜¯å¦ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ— ç¼–é€ å†…å®¹
        3. æ¥æºæ ‡æ³¨ï¼šæ˜¯å¦æ³¨æ˜ä¿¡æ¯æ¥æºéƒ¨é—¨å’Œæ—¶é—´
        4. åˆè§„æ€§ï¼šæ˜¯å¦ç¬¦åˆæ”¿åŠ¡æ²Ÿé€šè§„èŒƒï¼Œæ— æ•æ„Ÿä¿¡æ¯

        ## å¾…æ ¡éªŒå†…å®¹
        ä¸Šä¸‹æ–‡ï¼š{context[:1000]}...
        å›ç­”ï¼š{answer}

        ## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
        {{
            "relevance_score": 0.0-1.0,
            "accuracy_score": 0.0-1.0,
            "attribution_score": 0.0-1.0,
            "compliance_score": 0.0-1.0,
            "overall_score": 0.0-1.0,
            "suggestion": "ä¼˜åŒ–å»ºè®®ï¼ˆå¯é€‰ï¼‰"
        }}
        """

        try:
            response = Generation.call(
                model=self.model_name,
                prompt=validate_prompt,
                temperature=0.1,
                max_tokens=300,
                result_format='text'
            )

            if response.status_code == 200:
                validate_result = json.loads(response.output.choices[0].message.content)
                return validate_result
            else:
                raise Exception(f"è´¨é‡æ ¡éªŒå¤±è´¥: {response.code}")

        except Exception as e:
            logger.error(f"âŒ å›ç­”è´¨é‡æ ¡éªŒå¤±è´¥: {e}")
            return {
                "relevance_score": 0.0,
                "accuracy_score": 0.0,
                "attribution_score": 0.0,
                "compliance_score": 0.0,
                "overall_score": 0.0,
                "suggestion": f"æ ¡éªŒå¤±è´¥ï¼š{str(e)}"
            }

    async def generate_response(
        self,
        query: str,
        context: str,
        history: List[Dict] = None,
        decision: Optional[AgentDecision] = None,
        stream: bool = False
    ) -> Dict[str, any]:
        """
        å¢å¼ºç‰ˆç”Ÿæˆå›ç­”ï¼ˆç»“åˆAgentå†³ç­–ï¼‰
        """
        start_time = datetime.now()

        try:
            # å¦‚æœæ²¡æœ‰å†³ç­–ç»“æœï¼Œå…ˆæ‰§è¡Œæ„å›¾åˆ†æ
            if not decision:
                decision = await self.analyze_query_intent(query, history)

            # æ„å»ºAgentä¼˜åŒ–åçš„Prompt
            prompt = self.build_agent_rag_prompt(query, context, decision, history)

            logger.debug(f"Agentic Prompté•¿åº¦: {len(prompt)}å­—ç¬¦")
            logger.info(f"Agentå†³ç­–ç±»å‹: {decision.decision_type}, æ£€ç´¢ç­–ç•¥: {decision.retrieval_strategy}")

            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            if stream:
                return await self._generate_stream(prompt)
            else:
                generation_result = await self._generate_once(prompt, start_time)

                # å›ç­”è´¨é‡æ ¡éªŒ
                quality_check = await self.validate_answer_quality(
                    generation_result["answer"], query, context
                )

                # æ•´åˆæ ¡éªŒç»“æœ
                generation_result["quality_check"] = quality_check
                generation_result["agent_decision"] = decision.model_dump()

                return generation_result

        except Exception as e:
            logger.error(f"âŒ Agentic LLMç”Ÿæˆå¤±è´¥: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "quality_check": {"overall_score": 0.0},
                "agent_decision": {"decision_type": "cannot_answer"}
            }

    async def _generate_once(self, prompt: str, start_time: datetime) -> Dict[str, any]:
        """ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å›ç­”"""
        response = Generation.call(
            model=self.model_name,
            prompt=prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            top_p=self.top_p,
            result_format='message'
        )

        if response.status_code == 200:
            answer = response.output.choices[0].message.content

            return {
                "answer": answer,
                "usage": {
                    "prompt_tokens": response.usage.input_tokens,
                    "completion_tokens": response.usage.output_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "model": self.model_name,
                "finish_reason": response.output.choices[0].finish_reason,
                "response_time": (datetime.now() - start_time).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.code} - {response.message}")

    async def _generate_stream(self, prompt: str) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        response = Generation.call(
            model=self.model_name,
            prompt=prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            top_p=self.top_p,
            stream=True,
            result_format='message'
        )

        for chunk in response:
            if chunk.status_code == 200:
                if hasattr(chunk.output, 'choices') and chunk.output.choices:
                    content = chunk.output.choices[0].message.content
                    if content:
                        yield content
            else:
                yield f"é”™è¯¯: {chunk.code} - {chunk.message}"

    async def initialize(self) -> None:
        """åˆå§‹åŒ– LLM æœåŠ¡èµ„æº"""
        # é¢„çƒ­ï¼šæ‰§è¡Œä¸€æ¬¡ç®€å•çš„ç”Ÿæˆ
        await self.generate_response(
            query="",
            context="Hello",
            history=None,
            stream=False
        )
        logger.info("âœ… LLM Service é¢„çƒ­å®Œæˆ")


# å·¥å…·å‡½æ•°
def get_llm_service() -> LLMService:
    """è·å–Agentic LLMæœåŠ¡å•ä¾‹å®ä¾‹"""
    return LLMService()

async def generate_agentic_rag_response(
    query: str,
    context: str,
    history: List[Dict] = None,
    decision: Optional[AgentDecision] = None
) -> Dict[str, any]:
    """å¿«é€Ÿç”ŸæˆAgentic RAGå›ç­”"""
    service = get_llm_service()
    return await service.generate_response(query, context, history, decision)
