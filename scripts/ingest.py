import os
import sys
import pandas as pd
import hashlib
from tqdm import tqdm
from pymilvus import MilvusClient
from sentence_transformers import SentenceTransformer
import torch

sys.path.append(os.getcwd())

from app.core.config import settings

# ================= é…ç½®åŒºåŸŸ =================
BATCH_SIZE = 16  
MODEL_PATH = str(settings.MODEL_PATHS["embedding"])
DB_PATH = str(settings.MILVUS_DB_PATH)
COLLECTION_NAME = settings.COLLECTION_NAME
DATA_PATH = str(settings.RAW_DATA_PATH)

def get_device():
    if torch.cuda.is_available():
        return "cuda"
    return "cpu"

def generate_doc_id(question, answer):
    """
    åŸºäºå†…å®¹çš„å“ˆå¸Œç”Ÿæˆã€‚
    å°† é—®é¢˜+å›ç­” æ‹¼æ¥åè®¡ç®— MD5ï¼Œæˆªå–å‰ 16 ä½ä½œä¸º IDã€‚
    ä¿æŒä¸è¿ç§»è„šæœ¬é€»è¾‘ä¸€è‡´ã€‚
    """
    # ç¡®ä¿è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢æŠ¥é”™
    raw_str = str(question) + str(answer)
    # è®¡ç®— MD5
    hash_object = hashlib.md5(raw_str.encode('utf-8'))
    # è·å– 16è¿›åˆ¶å­—ç¬¦ä¸²ï¼Œæˆªå–å‰16ä½
    return hash_object.hexdigest()[:16]

def init_milvus(client):
    """åˆå§‹åŒ–æ•°æ®åº“é›†åˆ Schema"""
    if client.has_collection(COLLECTION_NAME):
        print(f"æ£€æµ‹åˆ°é›†åˆ {COLLECTION_NAME} å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤é‡å»º...")
        client.drop_collection(COLLECTION_NAME)

    print("ğŸ”¨ åˆ›å»ºæ–°é›†åˆ Schema...")
    client.create_collection(
        collection_name=COLLECTION_NAME,
        dimension=1024, # BGE-M3 ç»´åº¦
        metric_type="COSINE",
        auto_id=True,
        enable_dynamic_field=True 
    )

def process_and_ingest():
    # 1. è¯»å–æ•°æ®
    if not os.path.exists(DATA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {DATA_PATH}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
        return

    print(f"ğŸ“– è¯»å–æ•°æ®: {DATA_PATH}")
    df = pd.read_excel(DATA_PATH)
    
    # åˆ—åæ˜ å°„
    rename_map = {}
    for col in df.columns:
        if "é—®æ”¿å†…å®¹" in col:
            rename_map[col] = "question"
        elif "å›å¤å•ä½" in col:
            rename_map[col] = "department"
        elif "å›å¤å†…å®¹" in col:
            rename_map[col] = "answer"
            
    df = df.rename(columns=rename_map)
    
    # æ£€æŸ¥æ˜¯å¦æ˜ å°„æˆåŠŸ
    required_cols = ['question', 'answer']
    if not all(col in df.columns for col in required_cols):
        print(f"âŒ åˆ—ååŒ¹é…å¤±è´¥ï¼å½“å‰åˆ—å: {df.columns.tolist()}")
        print("è¯·ç¡®ä¿ Excel åŒ…å«ï¼š'é—®æ”¿å†…å®¹' å’Œ 'å›å¤å†…å®¹'")
        return

    # æ¸…æ´—æ•°æ®
    df = df.dropna(subset=['question', 'answer'])
    df['question'] = df['question'].astype(str)
    df['answer'] = df['answer'].astype(str)
    df['department'] = df['department'].astype(str)
        
    print(f"æœ‰æ•ˆæ•°æ®é‡: {len(df)} æ¡")

    # 2. åŠ è½½æ¨¡å‹
    device = get_device()
    print(f"ğŸ“¥ åŠ è½½ Embedding æ¨¡å‹: {MODEL_PATH} ...")
    try:
        embed_model = SentenceTransformer(MODEL_PATH, device=device)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. åˆå§‹åŒ– Milvus
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    client = MilvusClient(DB_PATH)
    init_milvus(client)

    # 4. æ‰¹é‡å¤„ç†
    total_rows = len(df)
    print("ğŸš€ å¼€å§‹å‘é‡åŒ–å¹¶å…¥åº“...")
    
    for i in tqdm(range(0, total_rows, BATCH_SIZE), desc="Processing"):
        batch = df.iloc[i : i + BATCH_SIZE]
        
        texts_to_embed = batch['question'].tolist()
        answers = batch['answer'].tolist()
        departments = batch['department'].tolist()
        
        # ç”Ÿæˆå‘é‡
        vectors = embed_model.encode(texts_to_embed, normalize_embeddings=True)
        
        data_to_insert = []
        for j, question_text in enumerate(texts_to_embed):
            # === ç”Ÿæˆå”¯ä¸€ ID ===
            # ç›´æ¥åœ¨è¿™é‡Œç”Ÿæˆ doc_idï¼Œæ›¿ä»£åç»­çš„è¿ç§»è„šæœ¬
            current_answer = answers[j]
            doc_id = generate_doc_id(question_text, current_answer)

            # RAG ä¸Šä¸‹æ–‡
            rag_context = f"å¸‚æ°‘è¯‰æ±‚ï¼š{question_text}\nå®˜æ–¹å›å¤ï¼š{current_answer}"
            
            data_to_insert.append({
                "vector": vectors[j],
                "text": rag_context,            
                "department": departments[j],   
                "metadata": {                   
                    "doc_id": doc_id,
                    "question": question_text,
                    "answer": current_answer
                }
            })
            
        client.insert(COLLECTION_NAME, data_to_insert)

    print(f"\nğŸ‰ å…¥åº“å®Œæˆï¼æ•°æ®åº“: {DB_PATH}")

    # 5. éªŒè¯æµ‹è¯•
    test_query = "é›¨éœ²è®¡åˆ’ä»€ä¹ˆæ—¶å€™å‘ï¼Ÿ"
    print(f"\nğŸ” æµ‹è¯•æ£€ç´¢: '{test_query}'")
    query_vec = embed_model.encode([test_query], normalize_embeddings=True)
    
    res = client.search(
        collection_name=COLLECTION_NAME,
        data=query_vec,
        limit=2,
        output_fields=["text", "department"]
    )
    
    for rank, hit in enumerate(res[0]):
        print(f"\n--- Rank {rank+1} (Score: {hit['distance']:.4f}) ---")
        print(f"éƒ¨é—¨: {hit['entity'].get('department')}")
        print(f"å†…å®¹æ‘˜è¦: {hit['entity'].get('text')[:100]}...")

if __name__ == "__main__":
    process_and_ingest()