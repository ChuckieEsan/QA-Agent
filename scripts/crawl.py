#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³¸å·å¸‚ç½‘ç»œé—®æ”¿å¹³å°å®šå‘é¡µæ•°é‡‡é›†è„šæœ¬
æ”¯æŒï¼šæŒ‡å®šé¡µæ•°èŒƒå›´ã€æ–­ç‚¹ç»­ä¼ ã€ä»£ç†è½®æ¢ã€è¿›åº¦æ˜¾ç¤º
"""

import asyncio
import argparse
import json
import logging
import time
import random
import os
import re
import sys
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

sys.path.append(os.getcwd())
from app.core.config import settings
from app.utils.data import clean_text

import aiosqlite
from playwright.async_api import async_playwright, Page, Browser, TimeoutError as PWTimeout

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s',
    handlers=[
        logging.FileHandler(f'log/spider_{datetime.now().strftime("%Y%m%d")}.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ScrapingConfig:
    """çˆ¬å–é…ç½®"""
    start_page: int = 1
    end_page: int = 10
    max_retries: int = 3
    delay_min: float = 2.0
    delay_max: float = 5.0
    batch_size: int = 5
    proxy_timeout: int = 30
    page_timeout: int = 30000
    checkpoint_interval: int = 2  # æ¯Né¡µä¿å­˜æ£€æŸ¥ç‚¹
    
    @property
    def total_pages(self) -> int:
        return self.end_page - self.start_page + 1

@dataclass
class QuestionItem:
    """
    é—®æ”¿æ•°æ®ç»“æ„ï¼ˆæ ¹æ®è¦æ±‚è°ƒæ•´ï¼‰
    - dept: é—®æ”¿å¯¹è±¡ï¼ˆéƒ¨é—¨ï¼‰
    - question: é—®æ”¿å†…å®¹ï¼ˆHTMLï¼‰
    - category: æœ¬åœ°åç»­æ ‡æ³¨ï¼Œåˆå§‹ä¸ºNULL
    - ä¸åŒ…å«statuså­—æ®µ
    """
    id: str
    title: str
    dept: str
    question: str
    answer: str
    category: Optional[str] = None
    question_time: str = ""
    answer_time: str = ""
    url: str = ""
    crawl_time: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "title": self.title,
            "dept": self.dept,
            "question": self.question,
            "answer": self.answer,
            "category": self.category,
            "question_time": self.question_time,
            "answer_time": self.answer_time,
            "url": self.url,
            "crawl_time": self.crawl_time
        }

class ProxyRotator:
    """ä»£ç†æ± è½®æ¢ç®¡ç†å™¨"""
    
    def __init__(self, proxies: List[str]):
        self.proxies = proxies
        self.current_index = 0
        self.failed_proxies = set()
        self.success_count = {}
        self.lock = asyncio.Lock()
        
        for p in self.proxies:
            self.success_count[p] = 0
        
        if not self.proxies:
            logger.warning("âš ï¸  ä»£ç†æ± ä¸ºç©ºï¼Œå°†ä½¿ç”¨ç›´è¿ï¼ˆé£é™©ï¼šIPå¯èƒ½è¢«å°ï¼‰")
        else:
            logger.info(f"âœ… ä»£ç†æ± å·²åŠ è½½ {len(proxies)} ä¸ªä»£ç†")
    
    def get_next_proxy(self) -> Optional[str]:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨ä»£ç†ï¼ˆæ™ºèƒ½è½®è¯¢ï¼‰"""
        if not self.proxies:
            return None
            
        available = [p for p in self.proxies if p not in self.failed_proxies]
        if not available:
            logger.warning("âš ï¸  æ‰€æœ‰ä»£ç†å‡å¤±è´¥ï¼Œé‡ç½®ä»£ç†æ± ")
            self.failed_proxies.clear()
            available = self.proxies
            
        # ä¼˜å…ˆä½¿ç”¨æˆåŠŸç‡é«˜çš„ä»£ç†
        available.sort(key=lambda x: self.success_count[x], reverse=True)
        proxy = available[0]
        self.current_index = (self.current_index + 1) % len(available)
        return proxy
    
    def mark_success(self, proxy: Optional[str]):
        """æ ‡è®°ä»£ç†æˆåŠŸ"""
        if proxy and proxy in self.success_count:
            self.success_count[proxy] += 1
    
    def mark_failed(self, proxy: Optional[str]):
        """æ ‡è®°ä»£ç†å¤±è´¥"""
        if proxy:
            self.failed_proxies.add(proxy)
            logger.warning(f"âŒ ä»£ç†å·²æ ‡è®°å¤±è´¥: {proxy[:20]}...")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä»£ç†ç»Ÿè®¡"""
        return {
            "total": len(self.proxies),
            "available": len(self.proxies) - len(self.failed_proxies),
            "failed": len(self.failed_proxies),
            "top_proxy": max(self.success_count.items(), key=lambda x: x[1]) if self.success_count else None
        }

class DatabaseManager:
    """å¼‚æ­¥SQLiteæ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, db_path: str = "luzhou_wenzheng.db"):
        self.db_path = db_path
        
    async def init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS wenzheng (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    dept TEXT,
                    question TEXT,
                    answer TEXT,
                    category TEXT,
                    question_time TEXT,
                    answer_time TEXT,
                    url TEXT,
                    crawl_time TEXT
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            await db.execute("CREATE INDEX IF NOT EXISTS idx_dept ON wenzheng(dept)")
            await db.execute("CREATE INDEX IF NOT EXISTS idx_time ON wenzheng(question_time)")
            await db.execute("CREATE INDEX IF NOT EXISTS idx_crawl ON wenzheng(crawl_time)")
            await db.commit()
        logger.info(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    async def insert_item(self, item: QuestionItem):
        """æ’å…¥æ•°æ®ï¼ˆå­˜åœ¨åˆ™æ›¿æ¢ï¼‰"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("""
                INSERT OR REPLACE INTO wenzheng 
                (id, title, dept, question, answer, category, question_time, answer_time, url, crawl_time)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                item.id, item.title, item.dept, item.question, item.answer,
                item.category, item.question_time, item.answer_time, item.url, item.crawl_time
            ))
            await db.commit()
    
    async def exists(self, item_id: str) -> bool:
        """æ£€æŸ¥IDæ˜¯å¦å·²å­˜åœ¨"""
        async with aiosqlite.connect(self.db_path) as db:
            async with db.execute("SELECT 1 FROM wenzheng WHERE id = ?", (item_id,)) as cursor:
                return await cursor.fetchone() is not None
    
    async def get_stats(self) -> Dict[str, int]:
        """è·å–æ•°æ®ç»Ÿè®¡"""
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute("SELECT COUNT(*), COUNT(category) FROM wenzheng")
            total, categorized = await cursor.fetchone()
            return {
                "total_records": total,
                "categorized": categorized,
                "uncategorized": total - categorized
            }
    
    async def get_by_page_range(self, start_time: str, end_time: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®ï¼ˆç”¨äºéªŒè¯ï¼‰"""
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute(
                "SELECT * FROM wenzheng WHERE crawl_time BETWEEN ? AND ?", 
                (start_time, end_time)
            )
            columns = [description[0] for description in cursor.description]
            rows = await cursor.fetchall()
            return [dict(zip(columns, row)) for row in rows]

class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨"""
    
    def __init__(self, start_page: int, end_page: int):
        self.start_page = start_page
        self.end_page = end_page
        self.total_pages = end_page - start_page + 1
        
        self.current_page_num = 0      # å½“å‰å®é™…é¡µç ï¼ˆå¦‚ 113ï¼‰
        self.completed_count = 0       # å·²å®Œæˆé¡µæ•°ï¼ˆ1, 2, 3...ï¼‰
        self.success_items = 0
        self.failed_items = 0
        self.skipped_items = 0
        self.start_time = time.time()
        self.lock = asyncio.Lock()
    
    async def update(self, page_num: int, success: int = 0, failed: int = 0, skipped: int = 0):
        """æ›´æ–°è¿›åº¦ï¼ˆpage_num æ˜¯å®é™…é¡µç ï¼‰"""
        async with self.lock:
            self.current_page_num = page_num
            self.completed_count += 1    # æ¯è°ƒç”¨ä¸€æ¬¡ï¼Œå®Œæˆä¸€é¡µ
            self.success_items += success
            self.failed_items += failed
            self.skipped_items += skipped
    
    def display(self):
        """æ˜¾ç¤ºè¿›åº¦æ¡"""
        elapsed = time.time() - self.start_time
        
        if self.total_pages > 0:
            percent = (self.completed_count / self.total_pages) * 100
        else:
            percent = 0
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´ï¼ˆåŸºäºå¹³å‡é€Ÿåº¦ï¼‰
        if self.completed_count > 0:
            avg_time_per_page = elapsed / self.completed_count
            remaining_pages = self.total_pages - self.completed_count
            eta_seconds = avg_time_per_page * remaining_pages
            eta = str(timedelta(seconds=int(eta_seconds)))
        else:
            eta = "è®¡ç®—ä¸­..."
        
        # è¿›åº¦æ¡
        bar_length = 40
        filled = int(bar_length * min(percent, 100) / 100)  # min é˜²æ­¢è¶… 100%
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
        
        sys.stdout.write(f"\r\033[K")
        sys.stdout.write(f"è¿›åº¦: |{bar}| {percent:.1f}% "
                        f"({self.completed_count}/{self.total_pages}é¡µ) "
                        f"å½“å‰é¡µ:{self.current_page_num} "
                        f"æˆåŠŸ:{self.success_items} "
                        f"è·³è¿‡:{self.skipped_items} "
                        f"å¤±è´¥:{self.failed_items} | "
                        f"ç”¨æ—¶:{timedelta(seconds=int(elapsed))} "
                        f"å‰©ä½™:{eta}")
        sys.stdout.flush()
    
    def summary(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦"""
        elapsed = time.time() - self.start_time
        return {
            "ä»»åŠ¡èŒƒå›´": f"{self.start_page}-{self.end_page}",
            "æ€»é¡µæ•°": self.total_pages,
            "å·²å®Œæˆ": self.completed_count,
            "å½“å‰é¡µ": self.current_page_num,
            "æˆåŠŸå…¥åº“": self.success_items,
            "è·³è¿‡(å·²å­˜åœ¨)": self.skipped_items,
            "å¤±è´¥": self.failed_items,
            "æ€»è€—æ—¶": str(timedelta(seconds=int(elapsed))),
            "å¹³å‡é€Ÿåº¦": f"{self.success_items/max(elapsed/60, 1):.1f}æ¡/åˆ†é’Ÿ"
        }

        
class LZEPSpider:
    """çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, config: ScrapingConfig, proxy_rotator: Optional[ProxyRotator] = None):
        self.config = config
        self.db = DatabaseManager(db_path=settings.RAW_DATA_DB_PATH)
        self.proxy_rotator = proxy_rotator
        self.progress = ProgressTracker(config.start_page, config.end_page)
        self.base_url = "https://wen.lzep.cn"
        self.list_pattern = "/node/reply/{}.html"
        
    async def init(self):
        await self.db.init_db()
        
    async def create_browser_context(self, playwright, proxy: Optional[str] = None):
        """åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        browser_options = {
            "headless": True,
            "args": [
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",  # å…³é”®ï¼šWSLå†…å­˜å¤„ç†
                "--disable-gpu",            # å…³é”®ï¼šç¦ç”¨GPUåŠ é€Ÿ
                "--disable-web-security",
                "--disable-features=TranslateUI",
                "--disable-extensions",
                "--disable-plugins",
                "--single-process",         #  WSLå»ºè®®å•è¿›ç¨‹æ¨¡å¼
                "--no-zygote",             #  WSLå»ºè®®
            ]}
        
        if proxy:
            browser_options["proxy"] = {"server": proxy}
            
        browser = await playwright.chromium.launch(**browser_options)
        
        context = await browser.new_context(
            user_agent=random.choice([
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            ]),
            viewport={"width": 1920, "height": 1080},
            locale="zh-CN",
            timezone_id="Asia/Shanghai",
            ignore_https_errors=True,
        )
        
        # åæ£€æµ‹è„šæœ¬
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
        """)
        
        return browser, context
    
    async def random_delay(self):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(self.config.delay_min, self.config.delay_max)
        await asyncio.sleep(delay)
    
    async def fetch_list_page(self, page: Page, page_num: int) -> Tuple[List[Dict], bool]:
        """
        è·å–åˆ—è¡¨é¡µæ•°æ®
        è¿”å›: (items, has_more)
        """
        url = f"{self.base_url}{self.list_pattern.format(page_num)}"
        
        try:
            await page.goto(url, wait_until="networkidle", timeout=self.config.page_timeout)
            await self.random_delay()
            
            # æå–åˆ—è¡¨
            items = await page.locator("#content-list li").all()
            pattern = r"/wen/(\d+)\.html"
            urls = []
            for item in items:
                link_locator = item.locator("h4 a")
                href = await link_locator.get_attribute("href")
                match = re.search(pattern, href)
                urls.append({
                    "id": match.group(1),
                    "url": f"https://wen.lzep.cn{href}"
                })
            
            return urls, len(urls) > 0
            
        except PWTimeout:
            logger.error(f"â±ï¸  ç¬¬{page_num}é¡µåŠ è½½è¶…æ—¶")
            return [], False
        except Exception as e:
            logger.error(f"âŒ è·å–åˆ—è¡¨é¡µ {page_num} å¤±è´¥: {e}")
            return [], False
    
    async def parse_detail(self, page: Page, item: Dict) -> Optional[QuestionItem]:
        """è§£æè¯¦æƒ…é¡µ"""
        try:
            await page.goto(item["url"], wait_until="networkidle", timeout=self.config.page_timeout)
            await self.random_delay()
            
            question_element = page.locator(".troub-wrap")
            answer_element = page.locator(".return-wrap")

            data = dict()
            data["id"] = item["id"]
            data["url"] = item["url"]
            data["title"] = await question_element.locator("h4").text_content()
            data["dept"] = await question_element.locator(".info >> li", has_text="é—®æ”¿å¯¹è±¡").locator("span").text_content()
            data["question_time"] = await question_element.locator(".time").first.text_content()
            data["question"] = await question_element.locator(".content-text").first.text_content()
            
            data["answer_time"] = await answer_element.locator(".time").first.text_content()
            data["answer"] = await answer_element.locator(".content-text").first.text_content()
            data["category"] = None
            
            print(data)
            
            return QuestionItem(
                **data,
                crawl_time=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"âŒ è§£æè¯¦æƒ…é¡µå¤±è´¥ {item['url']}: {e}")
            return None
    
    async def crawl_single_page(self, playwright, page_num: int) -> Tuple[int, int, int]:
        """
        çˆ¬å–å•é¡µ
        è¿”å›: (æˆåŠŸæ•°, è·³éæ•°, å¤±è´¥æ•°)
        """
        proxy = self.proxy_rotator.get_next_proxy() if self.proxy_rotator else None
        browser = None
        success_count = 0
        skip_count = 0
        fail_count = 0
        
        for attempt in range(self.config.max_retries):
            try:
                browser, context = await self.create_browser_context(playwright, proxy)
                list_page = await context.new_page()
                detail_page = await context.new_page()
                
                items, has_more = await self.fetch_list_page(list_page, page_num)
                
                if not has_more:
                    return 0, 0, 0
                
                for item in items:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if await self.db.exists(item['id']):
                        skip_count += 1
                        continue
                    
                    detail = await self.parse_detail(detail_page, item)
                    if detail:
                        await self.db.insert_item(detail)
                        success_count += 1
                    else:
                        fail_count += 1
                    
                    # æ¯å¤„ç†5æ¡å°ä¼‘æ¯
                    if (success_count + fail_count) % 5 == 0:
                        await asyncio.sleep(random.uniform(1, 2))
                
                # æ ‡è®°ä»£ç†æˆåŠŸ
                if self.proxy_rotator:
                    self.proxy_rotator.mark_success(proxy)
                
                return success_count, skip_count, fail_count
                
            except Exception as e:
                logger.error(f"âŒ ç¬¬{page_num}é¡µç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {str(e)[:100]}")
                if self.proxy_rotator and proxy:
                    self.proxy_rotator.mark_failed(proxy)
                    proxy = self.proxy_rotator.get_next_proxy()
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                
            finally:
                if browser:
                    await browser.close()
        
        return success_count, skip_count, fail_count
    
    async def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        await self.init()
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–ï¼šç¬¬{self.config.start_page}é¡µ åˆ° ç¬¬{self.config.end_page}é¡µ")
        logger.info(f"ğŸ“Š é¢„ä¼°æ€»é¡µæ•°: {self.config.total_pages}ï¼Œä»£ç†æ± çŠ¶æ€: {self.proxy_rotator.get_stats() if self.proxy_rotator else 'æœªå¯ç”¨'}")
        
        async with async_playwright() as playwright:
            for page_num in range(self.config.start_page, self.config.end_page + 1):
                try:
                    success, skipped, failed = await self.crawl_single_page(playwright, page_num)
                    await self.progress.update(page_num, success, failed, skipped)
                    self.progress.display()
                    
                    # æ£€æŸ¥ç‚¹ä¿å­˜ï¼ˆæ¯Né¡µï¼‰
                    if page_num % self.config.checkpoint_interval == 0:
                        stats = await self.db.get_stats()
                        logger.info(f"\nğŸ’¾ æ£€æŸ¥ç‚¹ - å·²çˆ¬å–{page_num}é¡µï¼Œæ•°æ®åº“æ€»è®¡: {stats['total_records']}æ¡")
                    
                    # æ‰¹æ¬¡é—´é•¿å»¶è¿Ÿï¼ˆæ¯batch_sizeé¡µï¼‰
                    if page_num < self.config.end_page and page_num % self.config.batch_size == 0:
                        rest = random.uniform(8, 15)
                        logger.info(f"\nğŸ˜´ å·²å®Œæˆ{self.config.batch_size}é¡µï¼Œä¼‘æ¯{rest:.1f}ç§’...")
                        await asyncio.sleep(rest)
                        
                except KeyboardInterrupt:
                    logger.info(f"\nâ›” ç”¨æˆ·ä¸­æ–­ï¼Œå½“å‰è¿›åº¦: ç¬¬{page_num}é¡µ")
                    break
                except Exception as e:
                    logger.error(f"\nğŸ’¥ ç¬¬{page_num}é¡µå¤„ç†å¼‚å¸¸: {e}")
                    continue
            
            # æœ€ç»ˆç»Ÿè®¡
            self.progress.display()
            print()  # æ¢è¡Œ
            summary = self.progress.summary()
            db_stats = await self.db.get_stats()
            
            logger.info("=" * 60)
            logger.info("ğŸ“ˆ çˆ¬å–å®Œæˆç»Ÿè®¡:")
            for k, v in summary.items():
                logger.info(f"   {k}: {v}")
            logger.info(f"ğŸ“¦ æ•°æ®åº“æ€»è®¡: {db_stats['total_records']}æ¡ (å·²æ ‡æ³¨{db_stats['categorized']}æ¡)")
            logger.info("=" * 60)

def load_proxies() -> List[str]:
    """åŠ è½½ä»£ç†"""
    # 1. ç¯å¢ƒå˜é‡ PROXY_LIST (é€—å·åˆ†éš”)
    env_proxies = os.environ.get("PROXY_LIST", "")
    if env_proxies:
        return [p.strip() for p in env_proxies.split(",") if p.strip()]
    
    # 2. æ–‡ä»¶ proxies.txt
    try:
        with open("proxies.txt", "r", encoding='utf-8') as f:
            lines = [l.strip() for l in f if l.strip() and not l.startswith("#")]
            if lines:
                return lines
    except FileNotFoundError:
        pass
    
    # 3. æœ¬åœ°ä»£ç†ï¼ˆClashç­‰ï¼‰
    # return ["http://127.0.0.1:7890"]
    
    return []

def parse_page_range(page_arg: str) -> Tuple[int, int]:
    """è§£æé¡µæ•°å‚æ•°"""
    if '-' in page_arg:
        start, end = map(int, page_arg.split('-'))
        return min(start, end), max(start, end)
    else:
        page = int(page_arg)
        return page, page

def main():
    parser = argparse.ArgumentParser(
        description="æ³¸å·å¸‚ç½‘ç»œé—®æ”¿å¹³å°å®šå‘é¡µæ•°é‡‡é›†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --pages 1-10              # çˆ¬å–ç¬¬1åˆ°10é¡µ
  %(prog)s --pages 5-20 --proxy      # çˆ¬å–ç¬¬5åˆ°20é¡µï¼Œä½¿ç”¨ä»£ç†
  %(prog)s --start 1 --end 50        # çˆ¬å–ç¬¬1åˆ°50é¡µ
  %(prog)s --page 3                  # ä»…çˆ¬å–ç¬¬3é¡µ
  %(prog)s --pages 1-100 --delay 3-8 # è‡ªå®šä¹‰å»¶è¿Ÿ3-8ç§’
        """
    )
    
    # é¡µæ•°å‚æ•°ï¼ˆäº’æ–¥ç»„ï¼‰
    page_group = parser.add_mutually_exclusive_group(required=True)
    page_group.add_argument('--pages', '-p', type=str, help='é¡µæ•°èŒƒå›´ï¼Œå¦‚ "1-10" æˆ– "5"')
    page_group.add_argument('--start', '-s', type=int, help='èµ·å§‹é¡µ')
    page_group.add_argument('--page', type=int, help='å•é¡µæ¨¡å¼ï¼ŒæŒ‡å®šæŸä¸€é¡µ')
    
    parser.add_argument('--end', '-e', type=int, help='ç»“æŸé¡µï¼ˆä¸--starté…åˆä½¿ç”¨ï¼‰')
    parser.add_argument('--delay', '-d', type=str, default="2-5", help='å»¶è¿ŸèŒƒå›´ï¼Œå¦‚ "2-5"ï¼ˆç§’ï¼‰')
    parser.add_argument('--proxy', action='store_true', help='å¯ç”¨ä»£ç†æ± ï¼ˆä»proxies.txtæˆ–ç¯å¢ƒå˜é‡è¯»å–ï¼‰')
    parser.add_argument('--db', default="luzhou_wenzheng.db", help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch', '-b', type=int, default=5, help='æ¯æ‰¹æ¬¡é¡µæ•°ï¼ˆé»˜è®¤5ï¼‰')
    parser.add_argument('--retry', '-r', type=int, default=3, help='å¤±è´¥é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰')
    
    args = parser.parse_args()
    
    # è§£æé¡µæ•°
    if args.pages:
        start_page, end_page = parse_page_range(args.pages)
    elif args.page:
        start_page, end_page = args.page, args.page
    else:
        if args.end is None:
            parser.error("--start éœ€è¦é…åˆ --end ä½¿ç”¨")
        start_page, end_page = args.start, args.end
    
    # è§£æå»¶è¿Ÿ
    delay_parts = args.delay.split('-')
    delay_min = float(delay_parts[0])
    delay_max = float(delay_parts[1]) if len(delay_parts) > 1 else delay_min + 3
    
    # åŠ è½½ä»£ç†
    proxies = load_proxies() if args.proxy else []
    proxy_rotator = ProxyRotator(proxies) if proxies else None
    
    # åˆ›å»ºé…ç½®
    config = ScrapingConfig(
        start_page=start_page,
        end_page=end_page,
        delay_min=delay_min,
        delay_max=delay_max,
        batch_size=args.batch,
        max_retries=args.retry
    )
    
    # è¿è¡Œ
    spider = LZEPSpider(config, proxy_rotator)
    try:
        asyncio.run(spider.run())
    except KeyboardInterrupt:
        print("\n\nâ›” ç”¨æˆ·å¼ºåˆ¶é€€å‡º")
    except Exception as e:
        logger.exception(f"ç¨‹åºå¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()